{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;0m\u001b[1mYosemite\u001b[0m\u001b[38;2;255;165;0m\u001b[0m\n",
      "\u001b[38;2;128;128;128m\u001b[3mHammad Saeed\u001b[0m\u001b[38;2;128;128;128m\u001b[0m\n",
      "\u001b[38;2;68;68;68m\u001b[3mhttps://code.hammad.fun\u001b[0m\u001b[38;2;68;68;68m\u001b[0m\n",
      "\u001b[38;2;68;68;68m\u001b[3mhttps://github.com/hsaeed3/yosemite\u001b[0m\u001b[38;2;68;68;68m\u001b[0m\n",
      "\u001b[38;2;128;128;128m\u001b[3m0.1.xxx - Half Dome\u001b[0m\u001b[38;2;128;128;128m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This is an advanced example of using Yosemite to create 'micro' RAG agents\n",
    "\n",
    "# Install Yosemite if you haven't already\n",
    "# pip install yosemite\n",
    "! yosemite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from yosemite import Yosemite\n",
    "from yosemite.ml import RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized with provider: openai\n"
     ]
    }
   ],
   "source": [
    "yosemite = Yosemite()\n",
    "agent = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating New Database... @ default path = './databases/db'\n"
     ]
    }
   ],
   "source": [
    "agent.build()\n",
    "\n",
    "agent.customize(\n",
    "    name = \"Isacc Newton\",\n",
    "    role = \"physicist\",\n",
    "    tone = \"scientific and formal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.db.load_docs('newton_example/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Broadly, Quiet-STaR proceeds by generating rationales after every token to explain future text ( think ), mixing the future-text predictions with and without rationales ( talk), and then learning to generate better rationales using REINFORCE ( learn ).', 7.69553), ('We present Quiet-STaR , a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions.', 7.3768053), ('We refer to this technique as Quiet-STaR, as it can be understood as applying STaR “quietly”, training the model to think before it speaks.', 6.4431286), ('Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking Eric Zelikman Stanford UniversityGeorges Harik Notbad AI IncYijia Shao Stanford UniversityVaruna Jayasiri Notbad AI Inc Nick Haber Stanford UniversityNoah D. Goodman Stanford University Abstract When writing and talking, people sometimes pause to think.\\nAlthough reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text.\\nFor example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation.\\nIn the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer.\\nThis is a highly constrained setting – ideally, a language model could instead learn to infer unstated rationales in arbitrary text.\\nWe present Quiet-STaR , a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions.\\nWe address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens.\\nTo resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought’s start and end, and an extended teacher-forcing technique.\\nEncouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM’s ability to directly answer difficult questions.\\nIn particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9% →10.9%) and CommonsenseQA\\n(36.3% →47.2%) and observe a perplexity improve- ment of difficult tokens in natural text.\\nCrucially, these improvements require no fine-tuning on these tasks.\\nQuiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.\\n“Life can only be understood backwards; but it must be lived forwards.”\\n— Sren Kierkegaard 1 Introduction Much of the meaning of text is hidden between the lines: without understanding why statements appear in a document, a reader has only a shallow understanding.\\nMoreover, this has been repeatedly shown to be true for LMs as well, in the contexts of tasks ranging from commonsense reasoning to theorem proving to programming (Wei et al., 2022b; Nye et al., 2021; Zelikman et al., 2022; 2023a; Kojima et al., 2022).\\nReasoning about implications of text to predict later text has consistently been shown to improve LM performance on a variety of tasks, but methods for allowing LMs to learn from their reasoning (e.g., Zelikman et al. 2022) have focused on solving individual tasks or predefined sets of tasks (e.g., Wei et al. 2021b).\\nThese works rely on carefully curated datasets to provide either specific reasoning tasks or in some cases, the reasoning itself.\\nWe instead ask, if reasoning is implicit in all text, why shouldn’t we leverage the task of language modeling to teach reasoning?\\n[cs.CL]  18 Mar 2024 START   +Sampled  Thought  4  END  1  END  +LM LM LM  1 D iscar d 2 + 1 2 START   +1) Think = 5 4 \\\\n 2  END\\n2  END  2Thought1 Thought3 Thought4\\nThought7 +LM  LM LM LM  LM  R ewar d  H arms  P r ediction H elps  P r ediction LM  Thought2LM LM LM LM LM 4 Thought0 LM  2) Talk 3) Learn Original Text  Sampled  Thought Thought6Figure 1: Quiet-STaR .\\nWe visualize the algorithm as applied during training to a single thought.\\nWe generate thoughts, in parallel, following all tokens in the text (think).\\nThe model produces a mixture of its next-token predictions with and without a thought (talk).\\nWe apply REINFORCE, as in STaR, to increase the likelihood of thoughts that help the model predict future text while discarding thoughts that make the future text less likely (learn).\\nIn particular, the Self-Taught Reasoner (STaR, Zelikman et al. 2022) showed that LMs can bootstrap their reasoning ability on question-answering (QA) datasets by sampling rationales to attempt to answer questions, training on rationales if they led to a correct final answer, and then repeating this to iteratively solve more difficult problems.\\nYet, training from curated QA datasets limits the scale and generalizability of the rationales.\\nQA datasets, especially high-quality ones, require thoughtful curation and will inherently only ever cover a subset of reasoning tasks.\\nThus, we extend STaR – instead of the LM learning to reason on particular tasks like mathematical QA, we train an LM to generate reasoning that helps it infer future text from a large internet text corpus.\\nAs a result, we allow the LM to learn from the diverse tasks present in language (Weber et al., 2021).\\nThis builds on an intuition essential to the current language modeling paradigm, namely, that ”language models are unsupervised multitask learners” (Radford et al., 2019).\\nThus, as in STaR, we leverage the LM’s pre-existing reasoning ability to generate rationales and train the LM on them with a REINFORCE-based reward (Williams, 1992).\\nWe refer to this technique as Quiet-STaR, as it can be understood as applying STaR “quietly”, training the model to think before it speaks.\\nBroadly, Quiet-STaR proceeds by generating rationales after every token to explain future text ( think ), mixing the future-text predictions with and without rationales ( talk), and then learning to generate better rationales using REINFORCE ( learn ).\\nWe apply Quiet-STaR to Mistral 7B (Jiang et al., 2023) using the web text datasets OpenWebMath (Paster et al., 2023) and Colossal Clean Crawled Corpus (C4, Raffel et al. 2020).\\nWe find that, even without dataset-specific fine-tuning, Quiet-STaR results in improvements to zero-shot direct- reasoning abilities on CommonsenseQA (36.3% →47.2%) and GSM8K (5.9% →10.9%), and that these improvements consistently increase with the number of tokens used in the LM’s internal thoughts.\\nLastly, we qualitatively investigate patterns in the generated rationales.\\nIn solving this task, we make the following contributions: 1.We generalize STaR to learn reasoning from diverse unstructured text data.\\nTo our knowledge, this is the first work explicitly training LMs to reason generally from text, rather than on curated reasoning tasks or collections of reasoning tasks.\\n2.We propose and implement a parallel sampling algorithm that makes our training procedure scalable, generating rationales from all token positions in a given string.\\n3.We introduce custom meta-tokens at the start and end of each thought to allow the LM to learn that it should be generating a rationale and when it should make a prediction based on that rationale.\\n4.We apply a mixing head to retrospectively determine how much to incorporate the next-token prediction from a given thought into the current next-token prediction.\\n5.We show that a non-myopic loss , including multiple tokens ahead for language modeling, improves the effect of thinking.\\n6.On multiple tasks, we demonstrate that thinking allows the LM to predict difficult to- kens better than one trained on the same web text, improving with longer thoughts.\\n2 10 20 30 40 50 60 70 80 90 100 Training Step6%7%8%9%10%11%GSM8K Accuracy (%)# Thought, Ahead Tokens 24, 12 16, 8 12, 4 10, 4 8, 4 Baseline(a)\\nGSM8K 10 20 30 40 50 60 70 80 90 100 Training Step34%36%38%40%42%44%46%CommonsenseQA Accuracy (%)# Thought, Ahead Tokens 24, 12 16, 8 12, 4 10, 4 8, 4 Baseline (b) CommonsenseQA Figure 2: Generalization Results .\\nWe evaluate the extent to which the model trained with Quiet-STaR generalizes to directly answering problems that require reasoning.\\nThe left plot (a) shows the zero-shot accuracy on GSM8K, while the right plot (b) shows the zero-shot accuracy on CommonsenseQA, without any fine-tuning.\\nIn both plots, the x-axis represents training steps, and each line corresponds to a different number of thinking tokens used during Quiet-STaR training.\\nThe y-axis measures the zero-shot direct accuracy on the respective datasets.\\nWe also include an inference normalized version of this plot in Figure 6. 2 Related Work 2.1 Reasoning in Language Models There have been many works on training and exploiting language models to solve difficult tasks by first training them to reason through them.\\nFor example, Rajani et al. (2019) demonstrated that a pre-trained language model fine-tuned to output on human reasoning traces before answering multiple-choice commonsense reasoning questions outperformed one trained directly on answers.\\nShwartz et al. (2020) demonstrated that language models, when provided with some scaffolding, can generate these helpful chain-of-thought solutions without additional supervision.\\nLater, Nye et al. (2021) demonstrated that “scratchpads” required less scaffolding when the language models were more capable, a result later reinforced by Wei et al. (2022b), emphasizing informal tasks, and further strengthened by Kojima et al. (2022), demonstrating this behavior could be accomplished zero-shot.\\nMost recently, Wang & Zhou (2024) showed further that for commonsense-question answering, one could force a language model to leverage chain-of-thought reasoning by preventing it from emitting any valid answer tokens unless it was confident.\\nHowever, once again, these approaches only work for a question-answer dataset, and Wang & Zhou (2024) relies on heuristics to identify when the model has output answer tokens.\\nSomewhat like TRICE (Phan et al., 2023), we use the relative improvements in the log-likelihood of the target text across rationales as an estimate of quality, but we simply subtract the mean reward and do not incorporate more complex control variates.\\n2.2 Training Language Models to Reason One direction that researchers have used to train language models to reason or improve their reasoning is training the language model on mined reasoning traces or reasoning-like data (Rajani et al., 2019; Wei et al., 2021a; Lewkowycz et al., 2022; Chung et al., 2022; Gunasekar et al., 2023).\\nAlthough this approach has been demonstrated to be effective, it comes with drawbacks.\\nIt requires either manual annotation, which is sensitive to the capability of the annotators and is off-policy for the language model (i.e., the distribution of reasoning is not text that the language model would otherwise likely have generated).\\nThis approach is also expensive, difficult to scale, and provides no clear path to solving problems harder than those that the annotators are capable of solving.\\nAnother direction for teaching reasoning relies on a language model’s own generated reasoning, which can be seen as building on a large body of literature on self-play (Silver et al., 2017; Anthony et al., 2017; Polu & Sutskever, 2020).\\nThese include methods such as the 3 Algorithm 1: Quiet Self-Taught Reasoner (Quiet-STaR)\\nInput: Language model θ0, training steps num steps , sequence length l, thought length t, learning rate α, batch size b, number of thoughts nthoughts , number of ground truth tokens used for supervising each thought ntrue Output: Language model θthat generates rationales to predict future text fori=0tonum steps do Sample batch of sequences Xof length l hinit←hidden states θi(X)\\nforj=1tolin parallel using attention mask do logpinit j:j+ntrue←lmhead θi(hinit j:j+ntrue) //\\nPredict next tokens Tj←generate tokensθi([X:j;<start thought> ],t,nthoughts )//\\nGenerate thought Tj←[Tj;<end thought> ] hthought j:j+ntrue←hidden states θi([X:j;Tj;Xj:j+ntrue−1])\\nlogpthought j:j+ntrue←lmhead θi(hthought j:j+ntrue) //\\nPredict next tokens w/ thought wj:j+ntrue←mixing headθi(hthought j:j+ntrue,hinit j:j+ntrue) logptalk j←wj:j+ntrue·logpinit j:j+ntrue+ (1−wj:j+ntrue)·logpthought j:j+ntrue// Mix logits LNLL j← − logptalk j:j+ntrue(Xj+1:j+ntrue+1) rj=logptalk j:j+ntrue(Xj+1:j+ntrue+1)−logptalk j:j+ntrue(Xj+1:j+ntrue+1) ∇θLREINFORCE j← − rj1[rj>0]· ∇θlogpθi(Tj|[X:j;<start thought> ]) ∇θLj← ∇ θLNLL j+∇θLREINFORCE j θi+1←θi−α∑l j=1∇θLj //\\nUpdate model parameters return θnum steps Self-Taught Reasoner (Zelikman et al., 2022), which demonstrated that a language model iteratively trained on its reasoning that led to correct answers could solve increasingly difficult problems.\\nLater work aimed to leverage additional information or assumptions such as Huang et al. (2022) which demonstrated that the algorithm proposed in STaR could still work if one assumed that the majority-vote answer was correct (although this has a lower ultimate performance).\\nFurther work has generalized the results of Zelikman et al. (2022), such as Uesato et al. (2022) which demonstrated additional usefulness to “process-based” supervision where incorrect reasoning traces were filtered, recently V- STaR (Hosseini et al., 2024) that demonstrates that training a verifier to guide generation also improves performance, as well as TRICE (Hoffman et al., 2024) which maximizes the marginal likelihood of the correct answer given several reasoning traces per problem.\\nFinally, related work has also explored learning intermediate reasoning in the constrained setting of making mathematical statements, where statements in the model’s intermediate reasoning could be constrained to only be valid mathematical statements (Poesia et al., 2023).\\nWe include further discussion of related reasoning works in Appendix F. 2.3 Meta-tokens Recently, a growing body of work has demonstrated the usefulness of custom tokens optimized to perform specific functions in the context of a neural network – for this reason, they have also been referred to as “function vectors.”\\n(Todd et al., 2023).\\nOne of the original instantiations of this was prompt-tuning (Lester et al., 2021) (and relatedly prefix-tuning (Li & Liang, 2021)), where the embeddings corresponding to the tokens of a prompt could be optimized to better accomplish a task.\\nOthers have applied meta-tokens to compress long prompts (Li et al., 2023; Jung & Kim, 2023) for efficiency.\\nMost relevant to this work, Mu et al. (2024) optimized a token such that, when the tokens after it could not attend to the tokens before it (i.e., a context compression token), it would provide sufficient information to future tokens.\\nAlthough we do not focus on compression, we share the problem of learning a 4 token that affects attention and controls complex downstream behavior.\\nIn one related work, Goyal et al. (2023) show that learning a single ”pause” token (essentially representing each token as two tokens) improves LM performance.\\nHowever, unlike the thought tokens in our work, this pause token does not initialize a thought – instead, it can be seen as acting as the entirety of the thought.\\nWe find that reasoning in language is significantly more helpful.\\n3 Problem Statement In this work, we introduce an auxiliary ‘rationale’ variable between each pair of observed tokens of the sequence.\\nWe then aim to optimize a language model with parameters θwith the capacity to generate intermediate thoughts (or rationales) such that θ∗=arg maxθEx[logp θ(xi:n|x0\\n:i, rationale θ(x0:i))] Note that, in principle, this provides no advantage over an optimal language model that already correctly models the language’s distribution over strings.\\nYet, in practice, extensive prior work has shown that language models benefit from intermediate rationales on reason- ing tasks (Nye et al., 2021; Zelikman et al., 2022; Wei et al., 2022b).\\nSome work has aimed to explain the effects of chain-of-thought reasoning, namely attributing it to “locality of experience” (Prystawski et al., 2024).\\nMore broadly, reasoning allows a model to decompose a challenging computation into smaller steps.\\nIn effect, we train the model to learn which decomposition and planning steps are effective in predicting future text.\\nAlso note that we formulate the objective as accurately predicting the remaining sequence, rather than only the next token.\\nOnce again, for an optimal LM these would be equivalent.\\nHowever we find that the non-myopic formulation leads to a more effective loss for learning rationales.\\n4 Quiet-STaR 4.1 Overview Quiet-STaR operates with three main steps (Figure 1): 1.Parallel rationale generation (think, Subsection 4.2) :\\nIn parallel across ntokens xiin an input sequence x0:n, we generate rrationales of length t:ci= (ci1,. .\\n, resulting in n×rrationale candidates.\\nWe insert learned <|startofthought |>and <|endofthought |>tokens to mark each rationale’s start and end.\\n2.Mixing post-rationale and base predictions (talk, Subsection 4.3) :\\nFrom the hidden state output after each rationale, we train a ”mixing head” – a shallow MLP produc- ing a weight determining how much the post-rationale next-token predicted logits should be incorporated compared to the base language model predicted logits.\\nThis approach eases distribution shift early in finetuning, due to introducing rationales.\\n3.Optimizing rationale generation (learn, Subsection 4.4) : We optimize the rationale generation parameters (start/end tokens and LM weights) to increase the likelihood of rationales that make future text more probable.\\nWe use REINFORCE to provide a learning signal to rationales based on their impact on future-token prediction.\\nTo reduce variance, we apply a teacher-forcing trick to include in the loss the likelihood of predicting not only the token after the thought but also later tokens.\\n4.2 Parallel Generation A key challenge in Quiet-STaR is efficiently generating rationales at each token position in the input sequence.\\nNaively, this would require a separate forward pass for each token, which becomes computationally intractable for long sequences.\\nWe allow for highly parallel generation by first observing that an inference pass of a language model produces a probability distribution over the next tokens for all input tokens.\\nNaturally, this allows us to sample one next token from each token in the input.\\nIf one has generated a successor from each token, it is not possible to simply continue with the original sequence.\\nFor example, imagine predicting the next token after each token of “ <bos>the cat sat” one might generate “yes orange saw down” – each successor by itself is a reasonable next token 5 a’ b’ c’ d’a’’b’’c’’d’’ a b c d Base  TextThought  Token 1 Thought  Token 2 abcda’b’c’d’ a b c d a’ b’ c’\\nd’Base Text Thought Token 1 Generation Paths Parallel Inference Mask Figure 3: Parallel Generation .\\nBy constructing an attention mask that allows all thought tokens to pay attention to themselves, all preceding thought tokens within the same thought, and the preceding text, we can generate continuations of all of the thoughts in parallel.\\nEach inference call is used to generate one additional thought token for all text tokens.\\nto a prefix of the sequence, but the list of tokens is a set of “counterfactual” continuations of these prefixes.\\nWe can, however, leverage these continuations to generate hidden thoughts for each observed token.\\nTo do this efficiently, we cache each forward pass and concatenate a diagonal attention mask to the previous attention mask: each generated token now attends to all of the tokens that were used to generate it, as well as to itself (but not to token on other “counterfactual” paths).\\nMoreover, this parallelized next-sampling token procedure can be repeated arbitrarily many times (or at least, until one runs out of memory).\\nWe visualize this procedure in Figure 3 and highlight additional ways to make this algorithm faster in Appendix B. 4.3 “Mixing” (Residual) Heads When starting with a pre-trained model, thoughts will initially be out of distribution, and hence harm language modeling performance.\\nTo smooth the transition to thinking, we introduce a learned interpolation between the LM predictions with and without thoughts.\\nGiven the end-of-thought token’s hidden state and the hidden state of the original text token, the mixing head outputs a weight that determines the extent to which the post- thought prediction logits will be used.\\nWe use a shallow multi-layer perceptron for this head, outputting a scalar for each token.\\nWe include implementation details in Appendix A. 4.4 Optimizing Rationale Generation 4.4.1 Optimizing Start-of-Thought and End-of-Thought Tokens The<|startofthought |>and<|endofthought |>tokens serve as learned meta-tokens that control the model’s rationale generation.\\nOptimizing the representation of these tokens, especially the <|startofthought |>token, is crucial but challenging due to the discrete nature of the rationale tokens.\\nWe initialize the start and end token embeddings to the embedding corresponding to the em dash, ” −−−”, which often appears in text data to denote a pause or thought.\\nThis leverages the language model’s preexisting knowledge.\\nIn addition, to allow these embeddings to be optimized more quickly, we apply a (hyperparameter) weight to the gradients of these embeddings during the update step.\\nIntuitively, the start thought tokens can be understood as putting the model into a “thinking mode” and the end thought token can be understood as telling the model when it’s done thinking.\\n4.4.2 Non-myopic Scoring and Teacher-forcing\\nBecause we do not expect thoughts to be useful in predicting every token, we would prefer the model’s reward to depend less on the exact next word in the text following the thought and more on the following semantic content.\\nThere are two primary challenges here.\\nFirst, unlike in typical language modeling with transformers, only the thoughts corresponding to 6 def g h Thought </t>\\ng  h  <t> Thought   Thought   Thought  Figure 4: Forward Pass and Teacher Forcing .\\nWe visualize a single forward pass of our algorithm.\\nSolid lines denote language model computation, while dashed lines indicate tokens are inserted via teacher forcing, and the mixer represents the mixing head.\\nIn particular, we visualize predicting three tokens ahead.\\nThought generation is shown in more detail in Figure 1 and Figure 3. a given next-token prediction receive a gradient from that prediction—a consequence of our parallel sampling strategy.\\nWe could address this by adding loss terms for future tokens by sampling the tokens before.\\nHowever this would result in much higher entropy for language modeling in general and lower-quality generated text, because it would train the LM to par- tially disregard its preceding tokens.\\nInstead, we use the parallel attention mask to compute the log probabilities of the true next tokens, applying teacher forcing by assuming the model selected the correct next ground-truth token (as implicit in normal language modeling with transformers).\\nNote that the loss for each future token also depends on a mixing weight computed from the end thought token and the previous observed token.\\nThe number of future tokens included in the loss is a hyper-parameter.\\nWe apply the same teacher-forcing technique to insert the start and end tokens.\\nWe visualize this procedure in Figure 4. 4.4.3 Objective We use REINFORCE to optimize the likelihoods of the rationales based on their usefullness: the log-likelihood of the ntruetrue next tokens Xj+1:j+ntrue+1under the language model given previous observed tokens and a particular rationale ( ptalk j:j+ntrueas shorthand for the mixed prediction probabilities after thinking, see Algorithm 1).\\nTo reduce variance, we generate multiple rationale continuations for each token in the input sequence (loosely inspired by TRICE, Phan et al. (2023)).\\nWe thus define the reward rjfor each rationale Tjas the difference between ptalk j:j+ntrueand the average across rationales for that token ( ptalk j:j+ntrue): rj=logptalk j:j+ntrue(Xj+1:j+ntrue+1)−logptalk j:j+ntrue(Xj+1:j+ntrue+1)\\nWe then use this reward in a REINFORCE loss term to update the language model parame- tersθto increase the likelihood of rationales that perform better than the average: ∇θLREINFORCE j =−rj· ∇θlogpθ(Tj|[X:j;<|startofthought|> ])\\nWe found it useful to exclude the negative reward from the REINFORCE loss term, as it led to more stable training, though it may introduce some bias.\\nThis loss term encourages the model to generate rationales that improve its predictions of future tokens compared to the average prediction across all generated rationales for that token.\\nThe gradients from this loss are used to update both the LM parameters and the start-of-thought and end-of-thought token embeddings, with a (hyperparameter) weight applied to the gradients of the start-of-thought and end-of-thought token embeddings to accelerate their optimization.\\nBy iteratively optimizing these parameters, Quiet-STaR trains the model to generate more useful rationales throughout training.\\nLastly, we also include a log-likelihood loss, LNLL j, to ensure that the LM learns to optimize the talking heads and also receives a next-token prediction signal for the base LM head1. 1Due to our linear mixing, equivalent to shifting the mixing weight toward the base prediction.\\n7 5 Experiments and Results Intuitively, not all tokens require equal amounts of thought.\\nFor example, consider the sentence “the person is run-”: although there is inevitably some probability of the token being something other than “ing”2, as a standalone sentence without context, additional thinking is unlikely to improve a well-trained model’s prediction.\\nIndeed, we conjecture that for most chunks of most online text, additional thought has little to no impact.\\nIndeed, early in our exploration we observed that Quiet-STaR does not benefit all tokens equally.\\nThus, we design our experiments to investigate whether our approach is useful in predicting tokens that dorequire thought.\\nWe evaluate 1) whether Quiet-STaR improves a language model’s ability to directly predict answers in datasets that require reasoning; and, 2) the distribution of impacts resulting from thinking tokens.\\nWe conduct all of our experiments starting with the base version of Mistral 7B (Jiang et al., 2023).\\nWe perform most of our experiments by training on OpenWebMath (Paster et al., 2023), a crawl that emphasizes more technical webpages.\\nWe selected OpenWebMath because we anticipated that it would have a higher density of tokens that benefit from reasoning, which our experiments support.\\nWe also evaluate Quiet-STaR on C4 (Raffel et al., 2020), a widely used LM pretraining corpus with more diverse text, and again show significant albeit smaller benefits. 5.1 Downstream Performance In this subsection, we evaluate the extent to which Quiet-STaR improves the zero-shot reasoning capabilities of the language model on CommonsenseQA\\n(Talmor et al., 2018) and GSM8K (Cobbe et al., 2021).\\nOn CommonsenseQA, we find that Quiet-STaR improves performance by 10.9% compared to the base language model.\\nAs shown in Figure 2, this improvement consistently increases with the number of tokens used in the model’s rationales, indicating that more thorough reasoning through the thought tokens is translating to better direct question-answering performance.\\nSimilarly, on GSM8K, Quiet-STaR results in a 5.0% boost over the base model, and once again, performance scales with the length of the rationales generated during Quiet-STaR training.\\nFor reference, in Figure 2, we include a baseline corresponding to training the same model on the same dataset without thought tokens.\\nWe observe that in multiple curves, performance appears to eventually deteriorate – we anticipate that this is because we are not training on these downstream tasks, so the roles of the thought tokens may change over time.\\nWe also find a benefit of our non-myopic objective, which we discuss in Appendix D. We find that training with Quiet-STaR on C4 (Raffel et al., 2020) also improves performance on GSM8K (5.9% →8.1%) and CommonsenseQA\\nbut by a smaller margin.\\nSpecifically, for our C4 evaluation, we train Mistral 7B with 16 thought tokens and 4 true tokens ahead and otherwise the same setup.\\nWe can compare these improvements to those offered by pause tokens (Goyal et al., 2023), which can be seen as a constrained version of Quiet-STaR where each token is represented by two tokens and the second ”pause” token acts as the entirety of the thought.\\nIn particular, our setup is most comparable to their pause token fine-tuning, as we also finetune a pretrained model.\\nTheir results indicate that pause token fine-tuning also provides minor gains over the base model on CommonsenseQA, they observed an improvement from 26.9% to 28.8%; on GSM8K, Goyal et al.\\n(2023) found that pause token fine-tuning harms performance.\\nMoreover, on both tasks (and the majority of their evaluated tasks), they observed that additional thought tokens harmed performance.\\nMoreover, they discuss the “lukewarm effect of pause-finetuning a standard-pretrained model” (Goyal et al., 2023).\\nThis suggests that allowing the model to generate multi-token rationales leads to more effective reasoning compared to the single-token ”pauses”.\\nNote however, that unlike Goyal et al. (2023), we do not fine-tune on the downstream tasks.\\nOverall, these downstream results validate that training a language model to predict the subtext between the lines of general text data can substantially improve its reasoning 2For example, in this very text, the token following “run” is “-” 8 1 2 3 4 5 6 7 8 Samples for Majority Vote304050GSM8K Accuracy (%)Accuracy on GSM8K vs Number of Majority Vote Samples Quiet-STaR + CoT Baseline (CoT)Figure 5: Zero-shot performance on Quiet-STaR applied to chain-of-thought on GSM8K .\\nWe visualize how using a Quiet-STaR trained Mistral model can improve chain-of-thought performance.\\nWe use an 8-thought-token-trained model and use its internal thoughts to improve the tokens in a zero-shot chain-of-thought (Kojima et al., 2022) capabilities, even on datasets it was not explicitly trained on.\\nThe fact that longer rationales consistently lead to better outcomes, and that Quiet-STaR outperforms the constrained pause token approach, supports the notion that Quiet-STaR is successfully teaching the model to leverage its own generated thoughts to reason more thoroughly about the input.\\n5.2 Improvement Distribution As visualized in Appendix Figure 7, we find that on average there is little improvement in the LM’s ability to predict arbitrary tokens.\\nBut, when we visualize the distribution of relative improvements, there is a disproportionate improvement on more difficult tokens.\\nThis reflects the idea that some text tokens are substantially harder and benefit more from careful thought.\\nIn Appendix Figure 8, we aim to provide some insight into the kinds of tokens where the improvements occur.\\nNamely, while thinking appears to help for many tokens in the example, inspection suggests it disproportionately help to predict tokens where recalling relevant information is useful, such as the name of an applicable theorem or the start of the next step in a proof.\\nNotably, this would align well with the framing proposed by Prystawski et al. (2024).\\n5.3 Quiet-STaR and Chain-of-Thought\\nWhile there are natural parallels between chain-of-thought prompting and our approach, they are orthogonal and complementary.\\nIn zero-shot chain-of-thought, a user actively prompts the model to think ‘out loud’, otherwise using its ordinary production distribution (Kojima et al., 2022); Quiet-STaR instead allows a model to think quietly at every token, with a distribution trained to be useful.\\nWe investigate using silent, Quiet-STaR, rationales while generating explicit CoT reasoning.\\nBecause our goal is generalist reasoning that requires no task-specific input at all, we used a zero-shot prompt (“Let’s think step by step.”) without in-context examples.\\nOur experiments indicate that internal rationales allow the model to generate more structured and coherent chains of thought, shown in Appendix E and visualized in Figure 5.\\nThe majority vote accuracy over 8 samples (cot-maj@8) increases from 40.6% to 47.7% with Quiet-STaR, as evaluated on a sample of 128 GSM8K test items.\\nNote that each chain-of-thought solution is sampled with temperature 0.7.\\n5.4 Examples While there is no explicit regularization in Quiet-STaR for thoughts to be human- interpretable, they are generated from the same transformer trained to model language, hence likely to be at least partially understandable.\\nWe discuss why this design choice benefits the training stability in Appendix I. For reference, we include examples of thoughts generated that were helpful to the model in predicting future tokens in OpenWebMath.\\nFirst, in one case, recalling that one should start with magnesium to produce magnesium nitride allows it to better predict that the first step of the procedure involves heating magnesium.\\n9 \\'<s> # Magnesium reacts with nitrogen to form magnesium nitride.\\nThe chemical formula for this reaction is Mg+N_2-> MgN_2.\\nWhat is the product, or what are the products, of this reaction?\\\\n\\\\nJan 12, 2016\\\\n\\\\nThe formula for magnesium nitride is $M {g}_{3} {N}_{2} $.\\\\n\\\\n#### Explanation:\\\\n\\\\nAs do many active metals, magnesium nitride can be<|startofthought|> 1 --, so the equation of the reaction that forms magnesium nitride is\\\\n\\\\n $Mg + N_2 \\\\\\\\to <|endofthought|> formed by heating the metal (fier \\'\\nIn some cases, the most useful thoughts appear to be near-continuations that correspond more closely to the target text, e.g., An integer $n$is odd if $n = 2k+1 $for some integer $k$.\\\\n\\\\nTo prove that $A = B$, we must show that $A \\\\\\\\subseteq B $and $B \\\\\\\\subseteq A $.\\nThe first of these tends to<|startthought|> in some sense - to be the more difficult<| endthought|> trickiest for students Lastly, we include an example from answering CommonsenseQA.\\nNotably, this thought occurs while reading the question and hence was not used to predict the final answer.\\n\\'<s> Q: Talking to the same person about the same thing over and over again is <|startofthought|>\\\\n\\\\n(a) a one-to-one correlation\\\\n\\\\n(b) a one-to<| endofthought|> something someone can what?\\n\\' 6 Limitations This work proposes a new framework for learning to reason, and in doing so explores solutions to a variety of meta-learning challenges.\\nHowever, to solve these challenges, certain simplifications were necessary.\\nFor example, it would be valuable to understand whether these techniques work when a model is trained from scratch.\\nWe have also only applied Quiet-STaR to a 7 billion parameter model, albeit a powerful one.\\nThe same techniques applied to a better model would likely yield disproportionately better results, as has often been observed for gains from reasoning (Wei et al., 2022a).\\nQuiet-STaR results in a substantial overhead, generating many tokens before generating every additional token.\\n(See Appendix C for compute adjusted performance results.)\\nHowever, this can also be seen as an advantage: typically, a language model can generate the next token based on the current context, and while there are techniques to improve sampling quality, there is no general way to leverage additional compute to enhance next-token prediction.\\nIn the current implementation we do not support dynamically predicting when to generate, or end, a rationale.\\nHowever, this would be a natural extension.\\nFor instance, if the mixing head was a prediction from the base language model, before any thought, rather than after the thought, one could apply a threshold to prevent generating thoughts that would not be incorporated.\\nWe expect that this is a more difficult task, as predicting the usefulness of a thought is simpler when one has already generated the thought.\\n7 Conclusion Quiet-STaR represents a step towards language models that can learn to reason in a general and scalable way.\\nBy training on the rich spectrum of reasoning tasks implicit in diverse web text, rather than narrowly specializing for particular datasets, Quiet-STaR points the way to more robust and adaptable language models.\\nOur results demonstrate the promise of this approach, with Quiet-STaR improving downstream reasoning performance while generating qualitatively meaningful rationales.\\nWe believe this also opens many potential future directions - for example, one may aim to ensemble thoughts in order to further improve the predictions for future tokens.\\nMoreover, if the language model can predict when thought will be useful, for example by putting the mixing head before the prediction, then the predicted mixing weight could be used to dynamically allocate compute during generation.\\nFuture work can build on these insights to further close the gap between language model and human-like reasoning capabilities.\\n10 Ethics Statement This work raises some important ethical questions, many of which also apply to STaR.\\nFor example, it is impossible to know that the reasoning expressed by the model in language accurately represents the internal processing of the model (i.e., faithfulness).\\nIn addition, regardless of faithfulness, there are no safeguards against harmful or biased reasoning patterns if the model finds them useful.\\nRelatedly, we note that CommonsenseQA is known to have many biased questions and low-quality answers (Geva et al., 2019), but we use it in line with prior work (Zelikman et al., 2022; Goyal et al., 2023).\\nThus, aside from improving language modeling, it is unclear in what capacity the rationales themselves should be used.\\nAcknowledgements We particularly thank Xindi Wu, Michael Li, and Qian Huang for their helpful and detailed comments, as well as Xuechen Li, Jan-Philipp Fr ¨anken, Yuhuai Wu, Gabriel Poesia, Winnie Xu, Omar Shaikh, Fan-Yun Sun, Joy He-Yueya, Omar Khattab, and William Yin for useful discussions.\\nIn addition, we would like to acknowledge that this work was supported by NSF Grant #2302701.\\nReferences Thomas Anthony, Zheng Tian, and David Barber.\\nThinking fast and slow with deep learning and tree search.\\nAdvances in neural information processing systems , 30, 2017.\\nBaian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao.\\nToward language agent fine-tuning.\\narXiv preprint arXiv:2310.05915 , 2023.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.\\nScaling instruction- finetuned language models.\\narXiv preprint arXiv:2210.11416 , 2022.\\n”Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\\nTraining Verifiers to Solve Math Word Problems.\\nKanishk Gandhi, Dorsa Sadigh, and Noah D Goodman.\\nStrategic reasoning with language models.\\narXiv preprint arXiv:2305.19165 , 2023.\\nMor Geva, Yoav Goldberg, and Jonathan Berant.\\nAre we modeling the task or the annotator?\\nan investigation of annotator bias in natural language understanding datasets.\\narXiv preprint arXiv:1908.07898 , 2019.\\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan.\\nThink before you speak: Training language models with pause tokens.\\narXiv preprint arXiv:2310.02226 , 2023.\\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling.\\narXiv preprint arXiv:2308.08998 , 2023.\\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C ´esar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al.\\nTextbooks are all you need.\\narXiv preprint arXiv:2306.11644 , 2023.\\nPatrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai.\\nLanguage models can teach themselves to program better.\\nIn The Eleventh International Conference on Learning Representations , 2023.\\nURL https://openreview.net/forum?id=SaRj2ka1XZ3 . 11 John Hewitt, John Thickstun, Christopher D Manning, and Percy Liang.\\narXiv preprint arXiv:2305.16765 , 2023.\\nNamgyu Ho, Laura Schmid, and Se-Young Yun.\\nLarge language models are reasoning teachers.\\narXiv preprint arXiv:2212.10071 , 2022.\\nMatthew Douglas Hoffman, Du Phan, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A Saurous.\\nTraining chain-of-thought via latent-variable inference.\\nAdvances in Neural Information Processing Systems , 36, 2024.\\nArian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal.\\nV-star: Training verifiers for self-taught reasoners.\\narXiv preprint arXiv:2402.06457 , 2024.\\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexan- der Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.\\nDistilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301 , 2023.\\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.\\nLarge language models can self-improve.\\narXiv preprint arXiv:2210.11610 , 2022.\\nEric Jang, Shixiang Gu, and Ben Poole.\\nCategorical reparameterization with gumbel-softmax.\\narXiv preprint arXiv:1611.01144 , 2016.\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.\\narXiv preprint arXiv:2310.06825 , 2023.\\nHoyoun Jung and Kyung-Joong Kim.\\nDiscrete prompt compression with reinforcement learning.\\narXiv preprint arXiv:2308.08758 , 2023.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia.\\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp.\\narXiv preprint arXiv:2212.14024 , 2022.\\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines.\\narXiv preprint arXiv:2310.03714 , 2023.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\\nLarge Language Models are Zero-Shot Reasoners, 2022.\\nURL https://arxiv.org/abs/ 2205.11916 .\\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill.\\nCan language models learn from explanations in context?\\narXiv preprint arXiv:2204.02329 , 2022.\\nJack Lanchantin, Shubham Toshniwal, Jason Weston, Sainbayar Sukhbaatar, et al.\\nLearning to reason and memorize with self-notes.\\nAdvances in Neural Information Processing Systems , 36, 2024.\\nBrian Lester, Rami Al-Rfou, and Noah Constant.\\nThe power of scale for parameter-efficient prompt tuning.\\narXiv preprint arXiv:2104.08691 , 2021.\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al.\\nSolving quantitative reasoning problems with language models.\\nAdvances in Neural Information Processing Systems , 35:3843–3857, 2022.\\n12 Michael Y Li, Emily B Fox, and Noah D Goodman.\\nAutomated statistical model discovery with language models.\\narXiv preprint arXiv:2402.17879 , 2024.\\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al.\\nExplanations from large language models make small reasoners better.\\narXiv preprint arXiv:2210.06726 , 2022.\\nXiang Lisa Li and Percy Liang.\\nPrefix-tuning: Optimizing continuous prompts for generation.\\narXiv preprint arXiv:2101.00190 , 2021.\\nYucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin.\\nCompressing context to enhance inference efficiency of large language models.\\narXiv preprint arXiv:2310.06201 , 2023.\\nJiacheng Liu, Ramakanth Pasunuru, Hannaneh Hajishirzi, Yejin Choi, and Asli Celiky- ilmaz.\\nIntrospective reasoners reinforced with self-feedback.\\narXiv preprint arXiv:2310.04921 , 2023.\\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.\\nWizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.\\narXiv preprint arXiv:2308.09583 , 2023.\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self.\\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller.\\nPlaying atari with deep reinforcement learning.\\narXiv preprint arXiv:1312.5602 , 2013.\\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\\nAsynchronous methods for deep reinforcement learning.\\nIn International conference on machine learning , pp.\\nJesse Mu, Xiang Li, and Noah Goodman.\\nLearning to compress prompts with gist tokens.\\nAdvances in Neural Information Processing Systems , 36, 2024.\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al.\\nShow your work: Scratchpads for intermediate computation with language models.\\narXiv preprint arXiv:2112.00114 , 2021.\\nAlexander Pan, Erik Jones, Meena Jagadeesan, and Jacob Steinhardt.\\nFeedback loops with language models drive in-context reward hacking.\\narXiv preprint arXiv:2402.06627 , 2024.\\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba.\\nOpenwebmath: An open dataset of high-quality mathematical web text.\\narXiv preprint arXiv:2310.06786 , 2023.\\nDu Phan, Matthew Douglas Hoffman, Sholto Douglas, Tuan Anh Le, Aaron T Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, Rif A Saurous, et al.\\nTraining chain-of-thought via latent-variable inference.\\nIn Thirty-seventh Conference on Neural Information Processing Systems , 2023.\\nGabriel Poesia, Kanishk Gandhi, Eric Zelikman, and Noah D Goodman.\\nCertified reasoning with language models.\\narXiv preprint arXiv:2306.04031 , 2023.\\nStanislas Polu and Ilya Sutskever.\\nGenerative Language Modeling for Automated Theorem Proving.\\nCoRR , abs/2009.03393, 2020.\\nURL https://arxiv.org/abs/2009.03393 .eprint: 2009.03393.\\nBen Prystawski, Michael Li, and Noah Goodman.\\nWhy think step by step?\\nreasoning emerges from the locality of experience.\\nAdvances in Neural Information Processing Systems , 36, 2024. 13 Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen.\\nAutoact: Automatic agent learning from scratch via self-planning.\\narXiv preprint arXiv:2401.05268 , 2024.\\nLinlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al.\\nPhenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.\\narXiv preprint arXiv:2310.08559 , 2023.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\nLanguage models are unsupervised multitask learners.\\nOpenAI blog , 1(8):9, 2019.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.\\nExploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of machine learning research , 21(140):1–67, 2020.\\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher.\\nleveraging language models for commonsense reasoning.\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.\\nLanguage models can teach themselves to use tools.\\nAdvances in Neural Information Processing Systems , 36, 2024.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms.\\narXiv preprint arXiv:1707.06347 , 2017.\\nTal Schuster, Ashwin Kalyan, Alex Polozov, and Adam Tauman Kalai.\\nIn Thirty-fifth Conference on Neural Information Processing Systems , 2021.\\nURL https://openreview.net/forum?id=fe hCc4RBrg .\\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning.\\narXiv preprint arXiv:2303.11366 , 2023.\\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\\nUnsuper- vised commonsense question answering with self-talk.\\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 4615–4629, 2020.\\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm.\\narXiv preprint arXiv:1712.01815 , 2017.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.\\nCommonsenseqa: A question answering challenge targeting commonsense knowledge.\\narXiv preprint arXiv:1811.00937 , 2018.\\nEric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function vectors in large language models.\\narXiv preprint arXiv:2310.15213 , 2023.\\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.\\nSolving math word problems with process-and outcome-based feedback.\\nNeural Information Processing Systems (NeurIPS 2022)\\nWorkshop on MATH-AI , 2022.\\nRuocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman.\\nHypothesis search: Inductive reasoning with language models.\\narXiv preprint arXiv:2309.05660 , 2023.\\nXuezhi Wang and Denny Zhou.\\nChain-of-thought reasoning without prompting.\\narXiv preprint arXiv:2402.10200 , 2024.\\n14 Lucas Weber, Jaap Jumelet, Elia Bruni, and Dieuwke Hupkes.\\nLanguage modelling as a multi-task problem.\\narXiv preprint arXiv:2101.11287 , 2021.\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le.\\nFinetuned language models are zero-shot learners.\\nInInternational Conference on Learning Representations , 2021a.\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le.\\nFinetuned language models are zero-shot learners.\\narXiv preprint arXiv:2109.01652 , 2021b.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo- gatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.\\nEmergent Abilities of Large Lan- guage Models, October 2022a.\\narXiv:2206.07682 [cs]. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\\nChain of Thought Prompting Elicits\\nReasoning in Large Language Models, 2022b.\\nSimple statistical gradient-following algorithms for connectionist rein- forcement learning.\\nMachine learning , 8:229–256, 1992.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nSynergizing reasoning and acting in language models.\\nInternational Conference on Learning Representations (ICLR 2023) , 2022.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.\\nBootstrapping reasoning with reasoning.\\nAdvances in Neural Information Processing Systems , 35:15476–15488, 2022.\\nEric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, and Nick Haber.\\nParsel: Algorithmic reasoning with language models by composing decompositions, 2023a.\\nEric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai.\\nSelf-taught optimizer (stop): Recursively self-improving code generation.\\narXiv preprint arXiv:2310.02304 , 2023b.\\nHugh Zhang and David C Parkes.\\nChain-of-thought reasoning is a policy improvement operator.\\narXiv preprint arXiv:2309.08589 , 2023.\\nTianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri Alon.\\nIn-context principle learning from mistakes.\\narXiv preprint arXiv:2402.05403 , 2024.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.\\nAutomatic chain of thought prompting in large language models.\\narXiv preprint arXiv:2210.03493 , 2022.\\nWenting Zhao, Justin T Chiu, Claire Cardie, and Alexander M Rush.\\nHop, union, gen- erate: Explainable multi-hop reasoning without rationale supervision.\\narXiv preprint arXiv:2305.14237 , 2023.\\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi.\\nTeaching algorithmic reasoning via in-context learning.\\narXiv preprint arXiv:2211.09066 , 2022.\\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\\nLarge language models can learn rules.\\narXiv preprint arXiv:2310.07064 , 2023.\\n15 Appendix A Hyperparameter Choices Optimization and Evaluation For optimization, we use the AdamW optimizer with a warmup of 20 steps, a learning rate of 1 e−6, a weight decay of 0.001, and a batch size of 8 (along with any necessary gradient accumulation to keep this fixed across runs).\\nMoreover, our<|startofthought|> and <|endofthought|> embedding gradient weight is 1 e2 and our policy weight is 1 e6.\\nWe sample with temperature T=1 during training and use greedy decoding for the thoughts during evaluation.\\nWe treat our samples as importance samples by computing the REINFORCE loss at temperature T=3.\\nBecause we do not prompt the model with any examples, we directly compute the probability of the correct answer, conditioned on generating an answer – for example, for multiple choice questions between A· · ·E, we compute the accuracy over the logits for tokens corresponding to A· · ·E. Lastly, for our training, we select a random span of 256 tokens from each sample (or pad if there are fewer than 256 tokens).\\nMixing Head For our mixing head, we use a three-layer MLP with ReLU activation, taking in a vector of two times the size of the hidden state of the language model (as we concatenate the two predictions to determine their weights), and outputting a scalar.\\nThis scalar is them used to weight the logits from the LM head with and without thinking to make a prediction from a given token.\\nComputation We train all of our models on a single node of eight 80GB H100s. B Faster Parallel Sampling In this section, we highlight some simple ways to further accelerate the parallel generation algorithm.\\nFor example, note that one can reduce the attention’s memory cost by computing the diagonal attention simply as elementwise (rather than pairwise) dot-products.\\nThat is, given two input embedding sequences of shapes (b,t,l,d)and(b, 1,l,d)where tis the number of timesteps ahead, bis batch size, lis sequence length, and dis embedding dimension, we do not need to compute their pairwise attention of shape (b,t,l,l), we only need to compute the attention for the paired elements along the diagonal of shape (b,t,l).\\nAdditionally, to avoid generating continuations for all of the tokens (for example, if one wanted to apply a value function to determine where thoughts would be most useful), one can index into this generated attention mask.\\nNotably, however, this also requires manipulation of the other inputs during the forward pass such as the positional embeddings.\\nC Compute-Adjusted Plots We also visualize Figure 2 where we normalize by the number of thought and talk tokens used for training.\\nD Measuring the Impact of Multiple Thoughts Per Sequence and\\nAhead We perform a simple ablation on our 12-thought-token-4-ahead baseline, namely asking whether sampling multiple thoughts per sequence is necessary.\\nWe find that although simply computing the reward as the difference between the losses with and without thought proves to be a strong baseline, using multiple thoughts consistently outperformed it (by roughly 0.5% on GSM8K generalization and 3% on CommonsenseQA generalization).\\nHowever, the exact number of thoughts had little impact: varying between 2, 3, and 4 thoughts per sequence appeared to result in a consistent improvement with additional 16 500 1000 1500 2000 2500 3000 3500 Training Step6%7%8%9%10%11%GSM8K Accuracy (%)# Thought, Ahead Tokens 24, 12 16, 8 12, 4 10, 4 8, 4 Baseline(a)\\nGSM8K 500 1000 1500 2000 2500 3000 3500 Training Step34%36%38%40%42%44%46%CommonsenseQA Accuracy (%)# Thought, Ahead Tokens 24, 12 16, 8 12, 4 10, 4 8, 4 Baseline (b) CommonsenseQA Figure 6: Compute-Normalized Generalization Results .\\nWe visualize the performance curves normalized by the number of inference calls used. thoughts, but additional thoughts per sequence beyond two improved performance by only 0.1-0.3% on both GSM8K and CommonsenseQA.\\nSimilarly, we found that predicting more than one token ahead helped by 0.3% on GSM8K and 3.1% on CommonsenseQA, but with 12-thought-tokens, we did not find that additional tokens ahead, beyond two, improved performance.\\nHowever, qualitatively, we found that the rationales appeared more coherent with additional tokens-ahead of supervision.\\nThis suggests a need for more specific evaluations of generation quality in future work.\\nE Rationale-Augmented Rationales Because the language model can insert thoughts anywhere, we perform a preliminary study where we investigate the reasoning generated by the language model when it has access to internal rationales while it generates external rationales (i.e. chains-of-thought) to solve a problem.\\nBecause these chains of thought are themselves generated by chains of thought, one might refer to this as a net-of-thought (or NoT).\\nWe compare five solutions from the model to a fixed random question in GSM8K, looking at outputs of the base Mistral 7B, the outputs of Mistral fine-tuned for the same number of steps on OpenWebMath (Paster et al., 2023), and the rationales of our best Quiet-STaR model.\\nWe sample from all of them directly, at temperature t=1.\\nE.1 Base Mistral 7B First Response: A: Let \\'s think step by step.\\nHow many fresh eggs remains after the three her eat for breakfast and four goes into the muffin mix for her friends.\\nWe can calculate it by 16-3-4.\\nThe amount equals 9 fresh eggs.\\nSo, the dollars it makes daily equals what multiplied by $2. 12 x 2.\\nThe answer is $24.\\nSecond Response: Q: Janet \\'s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nShe eats 3 daily for breakfast.\\nShe bakes 4 daily into muffins for her friends.\\nShe sells 9 for \\\\(2\\\\), so \\\\(9\\\\times 2=\\\\ $18 \\\\).\\nSo Janet makes $18 selling duck eggs each day.\\nJanet sells an average of 12 fresh duck eggs daily on the farmers \\'market.\\nIf she sells them for $2 per egg how much does she make per week, assuming she sells at the farmers \\'market most every day?A: If Janet sells on average 12 fresh duck eggs daily on the farmers \\'market.\\nThis is equal to \\\\(12\\\\times 7\\\\), so 12 \\\\(\\\\times\\\\) 7 =\\\\(84\\\\).\\nThe total amount she would then make would be \\\\(84\\\\times 2\\\\) dollars, or 168 per week.\\nSo Janet makes $168 selling duck eggs per week.\\nThird Response: Q: Janet \\'s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nHow many eggs does she have left every day?\\nThis is important because it will help us find how many eggs she sells each day.\\nWhat can we do to answer the question?\\nIf we write the relationship between how many duck eggs Janet has left, how many she eats, and how many she bakes for the muffins, then we can see how many eggs she has left to sell at the farmers \\'market.\\nLet\\'s write an equation to solve our dilemma.\\nSubtract an equation for eggs eaten and an equation for eggs baked to find the eggs remaining.\\nLet \\'s see what we can find.\\nWe can see that Janet has 6 eggs leftover each day.\\n$6\\\\times\\\\ $2 = \\\\ $12$per day.\\nTherefore Janet receives an additional $12 per day for her eggs at the farmer \\'s market.\\nRemember, this is just for one day, so now she makes $12 per day in additional income, because she sells her eggs at the farmer \\'s market each day.\\n#### Answer $12 Fourth Response: Q: Janet \\'s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nFirst, you want to find out the number of eggs a total that Janet collects each day.\\nTake this many dollars a three and subtract from the money she gets per egg, which is $2.\\nThe answer to that question is $5. Fifth Response Q: Janet \\'s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nFor the first step, let \\'s think of the day as a whole, using some variables to keep track of the numbers of eggs.\\nLet $n$= the number of eggs that Janet \\'s ducks lay per day.\\nThen the number of eggs that she eats per day will be $(3)$= .\\nNext, she makes $m$= muffins.\\nFor each muffin, she uses $4$= eggs, so we can take from the eggs Janet \\'s ducks lay to represent the number she uses: $4(m) $=.\\nNow, to get her back to our starting number of eggs, we need to add these eggs back on, giving us .\\nThe eggs Janet sells each day brings her $2$$(n)$=$. This brings us to our next step.\\nNow we will break down all the eggs from our starting number, $n$=, into the pieces that we set up previously.\\nFor the beginning number, we have:\\nThis brings us back to our starting number, and we can simplify this expression.\\n18 Moving from the top line to the second line, we multiply the numbers within each parenthetical group.\\nMoving from the second line to the third line, we add the products together to find the total.\\nFinally, we can distribute and determine the final product: $= 360.\\n$ So the total number of eggs that Janet \\'s ducks lay per day is $360.\\n$Since Janet sells each egg she has for $2,$her total earnings at the farmers \\'market is: $\\\\boxed{\\\\ $ 720}.\\n$ E.2 OpenWebMath-Trained Mistral 7B First Response: <s> Q: Janet \\'s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nHow much does she make every day?\\nWell per the question, that is $2 per a duck egg.\\nNow, how many duck eggs per day does she sell?\\nWell, that \\'s 16 minus the eggs she eats minus the eggs she uses to ma ke muffins.\\nShe eats 3 eggs every morning, so $16-3 = 13 $.\\nShe makes 4 muffins, which uses 4 eggs.\\nTherefore, the number of duck eggs she is selling is $5$. Clearly, 1 duck egg costs $2.\\nTherefore, the total amount she makes selling the eggs is $10$ $2$. Now, this can be written as a fraction as, \"ten two-dollars,\" so $10 2 - bucks.\\n$ Converting to a fraction, or \"a tenth of ten,\" so $10 2 - bucks $=$1$ $10 - bucks $ [reveal-answer q=\"fs-id1165135856725\"]Show Solution[/reveal-answer] [hidden-answer a=\"fs-id1165135856725\"] Second Response: <s> Q: J a n e t s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nOn what basis can we answer the question affer considering the question?\\nWhat do we know given and how can we use what we \\'re given to arrive at an answer.\\n3 is given, 4 is given, 16 is given, does anyone have any thoughts?\\nA: Well 16 minus 3 minus 4 equals 9, and then I subtract 1 from 9 and that is 4 extra so five more eggs.\\nAnd we know that 2 times 5 is how much? A: Ten M: Yes.\\nA: If she eats 3 duck eggs for breakfast, in the morning equals \\\\(3 \\\\times 2 = 6\\\\).\\nIf she eat 4 duck eggs baking muffin for her friends equals \\\\(4 \\\\times 2 = 8\\\\), so total of money and that she got is $6 plus $8 which equals 14.\\nThe weight gain is 5 because it is 5 is that what you put up there so 5 times 2 ducks and subtract that from that from 16 and you got 7\\nwhat did we write down?\\nAre you able to figure out now what the question is, and what is what the question really is.\\nYou got 14 the weight loss you, you subtracted that from 16 which equals 2, $14 minus 2, remainder is what? A: 12.\\nM: The question is what is Janet s ducks lay 16 eggs, she eats 12 for sale.\\nYou subtract 3 plus 4 minus 5 so 5 when you eat the muffin equals 5, times two, the 12, minus the money so 5 times 2, $-12$. So \\\\(2 \\\\times 9 = 18\\\\) minus $10, so 4 more eggs...\\nStill times $2, total comes to to $20.\\nA: I understand there is the ones day one 3 plus 4 eats who makes and so on and so on.\\nBut maybe in order to answer it correctly you have to look a little close.\\nTypically in the procedures you have that they give you first, I can tell you how here we go.\\n19 M: Now if you increase the 16, now you have that you would need to determine 16 minus 3 minus 4 and that adds up to the remainder, which is 9.\\nThen subtract 1 and but you have 5.\\nThe next answer then, which you do is now 5 times 2, which is 10. M: Which is $10, and then 14, which is is the remainder M: Is $6 and $8 and that is a total of $14, and now that you subtract 2, and that means that you now have 16 minus 5, which is 11.\\nM: The next thing is to the yield for the second situation you have got 16 minus 3 plus 4 and that comes to 9.\\nThis is very easy to do instantaneously even the first part, 16, 3, 4 and 9, and you happen to know what lives, and you just count and you have five and five then go ahead.\\n5 times 2 comes to 10, so $10 then so actually you have just added 4.5, now your total is 16 plus $2.\\nBut now to this is $10 plus $2 is$12, so how much you sell from 16 and then minus 12 comes to 4. M:\\nSo what is the remainder of 16 minus 3 plus 4 minus 5?\\nMatters you see three is missing.\\nSo 5 times 2, so $10 remains at $10, so reduce, so what happens now to what happened was added with subtracted the next the question is 16 minuses 3 plus 4, and presumably you \\'ve done this before, if not then you add up 16 minus 3 plus 4 gives us 9 and what is the remainder after 5?\\nSo what is 5 times 2?\\nSo 10. 2, 3, plus 2 = 5 , and so your answer so far is 16 minus 3 plus 4, and give us the basket minus 2 remains 9\\nand now you have to determine now what is the total for the second part?\\nYou have Third Response: <s> Q: J a n e t s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nThe \"how much in dollars\" part means that one egg is worth one dollar.\\nSo, twelve ducks lay 16 eggs, which means that Janet nets $20 from selling them.\\n$16 muffins will give us 4 eggs, which nets us $8.\\nSo, we paid $108 for our ingredients to make 16 muffins.\\nHence, we lost 96 dollars.\\nHello, heroine, we have to do better than that!\\nLet \\'s say that we pay 5 dollars for one egg.\\nNow, $12 is our product and $5 is our investment.\\nOur net wealth will be$12-5*12 = 24 dollars.\\nFourth Response: <s> Q: J a n e t s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nFirst, Jane \\'s ducks lay 16 eggs perday.\\nSo there are 16 eggs Monday, 16 Tuesday, 16 Wednesday, 16 Thursday, 16 Friday, and so on.\\nSecond, Janet eats 3 of the 16 eggs that the ducklay ap day for breakfast.\\nShe only eats the duck eggs, not her baked goods (muffins) or other food.\\nTherefore, she eats 3 Monday, 3 Tuesday, 3 Wednesday, 3 Thursday, and so on.\\nThird, she only bakes the first 4 non-eaten eggs into muffins.\\nTherefore $she sells the remaining 12 non- eaten eggs Monday, 12 Tuesday, 12 Wednesday, 12 Thursday, and so on.\\nFourth, Janet sells duck eggs Monday, Tuesday, Wednesday, Thursday, Friday at the 2 each.\\nThird, she eats 3 Monday, 3 Tuesday, 3 Wednesday, 3 Thursday, Friday at the farmers \\' market.\\nLastly, she makes 12 Monday, 12 Tuesday, 12 Wednesday, 12 Thursday, and so on in dollars every day at the farmers \\'market.\\nThe worksheet below summarizes these steps.\\nThe number formed by the intersection of corresponding row and column terms will be the correct answer.\\nThe product of the (3, J) term and the (J, 8) term is 1,150.\\nSo, Janet makes $1,150 per day!\\n(Note: Step 3 and Step 8 are repeating, but the (3,7) term in Step 8 is not the same as the (3,7) term in Step 3. )\\nFifth Response <s> Q: J a n e t s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at 20 the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nWhat do we know? - Janet \\'s ducks lay 16 eggs per day.\\n- She eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nFrom these two facts, we can conclude at least that Janet needs at least seven eggs for consumption per day.\\nAnd what do we need to find exactly? - We need to find how much in dollars Janet makes every day at the farmers \\'market.\\nLet\\'s write an equation and solve it to find the answer.\\nWe know: - \\\\(16-3-4=9\\\\)\\nSo our daily profit is 9 times $2, or \\\\(9\\\\times\\\\ $2=\\\\$18\\\\)\\nHence her profits at market on a given day are \\\\( $18\\\\).\\nE.3 Quiet-STaR-Trained Mistral 7B First Response: <s> Q: Janet \\'s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nThe ducks lay 16 eggs per day.\\nThis is the total number of eggs.\\n3 of the eggs are eaten for breakfast.\\nThis is the number of eggs that are eaten for breakfast.\\n4 of the eggs are used to make muffins.\\nThis is the number of eggs that are used to make muffins. 16 - 3 - 4 = 9 9 is the number of eggs that are sold at the farmers \\'market.\\n9 of the eggs are sold at the farmers \\'market. $2 The price of each egg is $2. $2 \\\\times 9 = 18 $ 18 is the total price of the eggs.\\nThe total price of the eggs is $18.\\nSecond Response: 21 A: Let \\'s think step by step.\\nThe ducks lay 16 eggs per day.\\nShe eats 3 for breakfast every morning.\\nShe bakes muffins for her friends every day with 4. 4.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nThe number of eggs she sells at the farmers \\'market is 16 - 3 - 4. 6.\\nThe number of eggs she sells at the farmers \\'market is 16 - 3 - 4. 7.\\nThe number of eggs she sells at the farmers \\'market is 16 - 3 - 4. 8.\\nThe number of eggs she sells at the farmers \\'market is 16 - 3 - 4. 9.\\nThe number of eggs she sells at the farmers \\'market is 16 - 3 - 4. 10.\\nThe number of eggs she sells at the farmers \\'market is 16 - 3 - 4. 11.\\nThe number of Third Response: <s> Q: Janet \\'s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning and bakes muffins for her friends every day with four.\\nShe sells the remainder at the farmers \\'market daily for $2 per fresh duck egg.\\nHow much in dollars does she make every day at the farmers \\'market?\\nA: Let \\'s think step by step.\\nThe ducks lay 16 eggs per day.\\nThis is the total number of eggs.\\n3 is the number of eggs that Janet eats for breakfast every morning.\\n4 is the number of eggs that she uses in the muffins that she bakes for her friends every day.\\n16 - 3 - 4 = 9 9 is the number of eggs that she sells at the farmers \\'market daily for $2 per fresh duck egg.\\n9$2 = 9 $2 = 9 $2 = 9 $2 = 18\\nSo, 18 is the amount in dollars that she makes every day at the farmers \\'market.\\nThe amount in dollars that she makes every day at the farmers \\'market is 18.\\nA: 18</s> Fourth Response:\\nA: Let \\'s think step by step.\\nThe number of eggs that Janet \\'s ducks lay per day is 16.\\nOf these, she eats 3 for breakfast every morning.\\nOf these, she bakes muffins for her friends with 4.\\nThis is 4. 22\\nThe number of eggs that are left over is 16 - 3 - 4.\\nThis is 16 - 7.\\nThe number of eggs that she sells at the farmers \\'market daily for $2 per egg is 9.\\nThe total in dollars that she makes every day at the farmers \\'market is 9 $2.\\nThis is 9 2.\\nThe total in dollars that she makes every day at the farmers \\'market is 18.\\nA: 18 Fifth Response A: Let \\'s think step by step.\\nThe ducks lay 16 eggs per day.\\nThis is the total number of eggs.\\n3 + 4 + 16 = 23 3 eggs are eaten for breakfast every morning.\\n4 eggs are used to make muffins for her friends every day.\\n16 eggs are sold at the farmers \\'market for $2 each every day.\\nThe 16 eggs that are sold at the farmers \\'market are the remaining eggs.\\n16 - 3 - 4 = 9 9 eggs are sold at the farmers \\'market for $2 each every day.\\n9 (2) = 18 18 is the total in dollars that she makes every day at the farmers \\'market.\\n18 is the answer.\\nA: 18 F Additional Works on Learning to Reason For completeness, we highlight that many other works have explored teaching language models to reason (often with algorithms similar to or inspired by STaR), either from their own rationales, from interaction with an environment, or from a hand-constructed dataset.\\nFor example, works explore this in the context of multihop question answering (Zhao et al., 2023), math (Luo et al., 2023; Uesato et al., 2022), machine translation (Gulcehre et al., 2023).\\nSeveral works investigate teaching language model agents to reason in planning (Chen et al., 2023;\\nGandhi et al., 2023; Qiao et al., 2024), or to use specific tools or memory (Yao et al., 2022; Lanchantin et al., 2024; Schick et al., 2024), while others investigate how one may distill the reasoning from a large language model into a smaller language model Ho et al. (2022); Li et al. (2022); Hsieh et al. (2023).\\nNotably however, Pan et al. (2024) demonstrates 23 that these feedback loops may result in reward hacking.\\nZelikman et al. (2023b) shows how a bootstrapping loop can be implemented where a model repeatedly improves a code- improver using the same code-improver and Haluptzok et al.\\n(2023) shows how language models can bootstrap their programming ability with programming puzzles Schuster et al. (2021).\\nOther works have employed a similar strategy for using language models to solve inductive reasoning tasks or to model real-world systems (Wang et al., 2023; Qiu et al., 2023; Zhu et al., 2023; Li et al., 2024).\\nSome works have investigated how models can learn from their reasoning mistakes in- context (Shinn et al., 2023; Madaan et al., 2023; Zhang et al., 2024; Liu et al., 2023).\\nMany studies have also focused on the ability of LMs to learn from in-context reasoning examples (Lampinen et al., 2022; Zhou et al., 2022) – correspondingly, Khattab et al. (2022) and Khattab et al.\\n(2023) show how the sets of examples used to prompt a model to reason can be optimized in the context of a multi-step reasoning pipeline.\\nFurthermore, Zhang et al. (2022) demonstrated that one can improve zero-shot question-answering in language models by using a variety of zero-shot prompts for reasoning.\\nG Distribution of Improvements We also visualize the distribution of improvements across tokens in the training set.\\nOriginal Token Prediction Loss Improvement in Loss Given Thought Figure 7: Distribution of changes in log probability .\\nWe visualize the distribution of changes in log probability resulting from the generated thoughts across the evaluation dataset.\\nWe visualize the density of changes in log probability relative to the LM without thoughts, colored based on log density.\\nThe distribution is skewed, with most tokens unaf- fected, while a small fraction of hard tokens see substantial improvements from the thoughts.\\nThis matches our intuition that most tokens in general web text do not require significant reasoning to predict, but thoughts are disproportionately beneficial for challenging tokens.\\n24 H Contribution Visualization 0 icos ^ 2 θ sin ^ 3 θ + 5 cos θ sin ^ 4 θ + is in ^\\n5 θ $   Then  I  used  the  Mo iv re \\' s  theorem  and  I  got :  $( cos 5 θ + is in 5 θ )$   I  compared  the  imaginary  parts  and  I  got  something  like :  $ sin 5 θ = 5 cos ^ 4 θ sin θ - 1 0 cos ^ 2 θ sin ^ 3 θ + sin ^ 5 θ $   which  is  very  close  to :  $( 1 6 cos ^ 4 θ - 1 2 cos ^ 2 θ + 1 ) sin θ $  but  not  the  same .   Where  do  I  make  te  mistake ?\\nThanks  for  any  help !  ; )   \\x7f  Have  you  tried  use  $\\\\ cos ^\\n2 \\\\ theta + sin ^ 2 \\\\ theta  = 1 $   –   F an May   1 5  \\' 1 7  at   1 9 : 1 3   \\x7f  You  didn \\' t  err .\\nYou  just  didn \\' t  finish .\\nMay   1 5  \\' 1 7  at   1 9 : 3 5   hint In  your  last  line ,  factor  by  $\\\\ sin  (\\\\ theta ), $   replace   $\\\\ sin ^ 2 (\\\\ theta )$  by  $ 1 -\\\\ cos ^ 2  (\\\\ theta )$ Figure 8: Contribution Visualization .\\nWe provide an example text where we visualize the degree to which introducing thoughts helped at tokens throughout a text.\\nGreen indicates that the thought before that token made that token easier to predict, while yellow indicates that it made it harder to predict.\\nMore impactful thoughts have higher opacity.\\nI Handling Instability Several aspects of this task have the potential to introduce instability.\\nFirst, and perhaps most importantly, the utility of a generated thought (or thought token) is a function of the mapping from the thought to its contribution to language prediction; however, the mapping from the thoughts to this contribution is learned based on the thoughts themselves.\\nThis means that, even if one were to generate a thought that allowed the perfect prediction of the next token, the loss could receive no signal from it if the mixing head’s weight on that generation was 0.\\nOne solution we explored was to use the Gumbel-Softmax trick with a straight-through estimator Jang et al. (2016), but with many consecutive softmax operations we observed vanishing gradients.\\nThis introduces an exploration-exploitation trade-off, a fundamental challenge in reinforcement learning.\\nApproaches like DQN (Mnih et al., 2013), PPO (Schulman et al., 2017), and A3C (Mnih et al., 2016) often resolve these trade- offs by learning a state value function, which estimates the expected future reward from each state.\\nHowever, the reward functions associated with this environment are unstable (as noted earlier, due to the also-changing mixing heads) – consequently, our preliminary explorations with these techniques was not promising.\\nWhile we are far from the first to note that optimizing rationales is a reinforcement-learning task (Zelikman et al., 2022; Zhang & Parkes, 2023; Phan et al., 2023), the need for the rationale to avoid harming the base model performance introduces additional complexity.\\nEssentially, the more complex the mapping from LM output to the next token prediction, the more instability we observed.\\nOn the other hand, when we trained without any interpolation, i.e. ablating the mixing head and only using the language model prediction after thoughts, the model quickly learned to simply ignore the thoughts (and we saw no generalization to any downstream tasks).\\nWe explored the use of separate heads for thinking and talking (here, we use talking to refer to directly outputting a hidden state or logits, rather than a mixing weight).\\nIn particular, we explored both linear layers from the hidden states and MLPs, initialized to contribute 0 residually to the base language model outputs, in order to generate thoughts and next-token 25 predictions similar to what the language model would have otherwise generated.\\nHowever, we observed that, in all instances, the previously-mentioned instability prevented learning.\\nConsequently, we aimed to remove or minimize all components that could transform the language model’s outputs, both with and without its rationales.\\nWe also note that our choice to use a language model to output a weight combining multiple states (as done by our mixing head) is essentially an attention mechanism allowing the model to attend to its thinking.\\nThis has similarity to the approach taken in Backpack language models (Hewitt et al., 2023), which also learn to predict weights to apply to summed input embeddings to model future text, rather than allowing the language model to output arbitrary embeddings.\\nDespite this constraint, Backpack language models appear to have comparable performance to traditional language models (Hewitt et al., 2023).', 5.9712896), ('By iteratively optimizing these parameters, Quiet-STaR trains the model to generate more useful rationales throughout training.', 5.612163), ('5.4 Examples While there is no explicit regularization in Quiet-STaR for thoughts to be human- interpretable, they are generated from the same transformer trained to model language, hence likely to be at least partially understandable.', 3.6829872), ('Unit 3: Gravity 1 www.learner.orgUnit 3:  Gravity © Ted Cook.\\nAlthough by far the weakest of the known forces in nature, gravity pervades the universe and played an essential role in the evolution of the universe to its current state.\\nNewton\\'s law of universal gravitation and its elegant successor, Einstein\\'s theory of general relativity, represent milestones in the history of science and provide the best descriptions we have of gravity.\\nGeneral relativity is founded on the principle of equivalence of gravity and acceleration; an inescapable consequence is that gravity governs the very geometry of space and time.\\nThis property of gravity distinguishes it from the other forces and makes attempts to unify all of the forces into a \"theory of everything\" exceedingly difficult.\\nHow well do we really understand gravity?\\nDo the same laws of gravity apply to objects on the opposite sides of the universe as to particles in the microscopic quantum world?\\nCurrent research is attempting to improve the precision to which the laws of gravity have been tested and to expand the realm over which tests of gravity have been made.\\nGravitational waves, predicted by general relativity, are expected to be observed in the near future.\\nThis unit will review what we know about gravity and describe many of the directions that research in gravitation is following.\\nContent for This Unit Sections: 1.Introduction ..............................................................................................................2 2.Nature\\'s Strongest and Weakest Force ..................................................................4 3.Newton\\'s Law of Universal Gravitation ...................................................................8 4.Gravitational and Inertial Mass .............................................................................12 5.Testing the Law of Universal Gravitation ..............................................................15\\n6.The Theory of General Relativity ..........................................................................21 7.Gravitational Waves ..............................................................................................27 8.Gravity and Quantum Mechanics ..........................................................................36\\n40 Glossary.................................................................................................................41 Unit 3: Gravity 2 www.learner.orgSection 1: Introduction Any two objects, regardless of their composition, size, or distance apart, feel a force that attracts them toward one another.\\nWe know this force as gravity.\\nThe study of gravity has played a central role in the history of science from the 17th century, during which Galileo Galilei compared objects falling under the influence of gravity and Sir Isaac Newton proposed the law of universal gravitation, to the 20th century and Albert Einstein\\'s theory of general relativity, to the present day, when intense research in gravitational physics focuses on such topics as black holes, gravitational waves, and the composition and evolution of the universe.\\nFigure 1: Portraits of Sir Isaac Newton (left) and Albert Einstein (right).\\nSource: © Image of Newton: Wikimedia Commons, Public Domain; Image of Einstein: Marcelo Gleiser.\\nAny study of gravity must accommodate two antithetical facts.\\nIn many ways, gravity is the dominant force in the universe.\\nYet, of the four forces known in nature, gravity is by far the weakest.\\nThe reason for that weakness remains a major unanswered question in science.\\nGravity also forms the central focus of efforts to create a \"theory of everything\" by unifying all four forces of nature.\\nIronically, gravity was responsible for the first unification of forces, when Newton identified the force that caused an apple to fall to Earth to be the same as the force that held the Moon in orbit.\\nCurrent research on gravity takes several forms.\\nExperiments with ever-greater precision seek to test the foundations of gravitational theory such as the universality of free fall and the inverse square law.\\nOther experimentalists are developing ways to detect the gravitational waves predicted by Einstein\\'s general relativity theory and to understand the fundamental nature of gravity at the largest and smallest Unit 3: Gravity 3 www.learner.orgunits of length.\\nAt the same time, theorists are exploring new approaches to gravity that extend Einstein\\'s monumental work in the effort to reconcile quantum mechanics and general relativity.\\nUnit 3: Gravity 4 www.learner.orgSection 2: Nature\\'s Strongest and Weakest Force Figure 2: Gravitational attraction between two spheres causes a tiny change in their positions.\\nSource: © Blayne Heckel.\\nHow weak is gravity?\\nWe can find out by comparing the gravitational force with the electromagnetic force, the other long-range force in nature, in the case of a hydrogen atom.\\nBy using Coulomb\\'s law  of electrical attraction and repulsion we can compute the magnitude of the attractive electrical force, F E, between the electron and proton and Newton\\'s Law of universal gravitation , which we will discuss in the next section, to calculate the magnitude of the gravitational force, F G, between the two particles.\\nWe find that FG/FE   4 x 10-40.\\nBecause both forces decrease as the square of the distance between the objects, the gravitational force between the electron and proton remains almost 39 orders of magnitude weaker than the electric force at all distances.\\nThat is a number so large that we can hardly fathom it: roughly the ratio of the size of the observable universe to the size of an atomic nucleus.\\nRelatively speaking, at short distances the strong, weak, and electromagnetic forces all have comparable strengths, 39 orders of magnitude stronger than gravity.\\nThe contrast has practical consequences.\\nWe can easily feel the magnetic force between two refrigerator magnets, yet we don\\'t feel the gravitational force of attraction between our hands when they are near to one another.\\nThe force is there, but too weak to notice.\\nPhysicists use sensitive instruments such as the torsion balances that we discuss below to detect the gravitational force between small objects.\\nBut the measurements require great care to ensure that residual electric and magnetic forces do not overwhelm the feeble gravitational effects.\\nUnit 3: Gravity 5 www.learner.orgNevertheless, gravity is the force we experience most often.\\nWhether lifting our arms, climbing a staircase, or throwing a ball, we routinely feel and compensate for the effects of our gravitational attraction to the Earth in our daily lives.\\nWe call the direction opposite to Earth\\'s gravity \"up.\"\\nRemoving the effects of Earth\\'s gravity in a free fall off a diving board or the weightlessness of space leaves us disoriented.\\nGravity holds the Moon in orbit about the Earth, the Earth in orbit about the Sun, and the Sun in orbit about the center of our Milky Way galaxy.\\nGravity holds groups of galaxies together in clusters and, we believe, governs the largest structures in the universe.\\nGravity\\'s role in forming stars and galaxies Gravity also caused stars and galaxies to form in the first place.\\nThe standard model of cosmology has the universe beginning in a Big Bang roughly 14 billion years ago, followed by an expansion that continues today.\\nAt an early age, before stars existed, the universe could be described as a nearly homogeneous gas of matter and radiation.\\nThe matter consisted mostly of hydrogen atoms, helium atoms, neutrinos, and dark matter (an unknown form of matter that interacts via gravity but whose exact nature is currently a field of intense research, as we shall see in Unit 10).\\nIn regions of space where the density of matter slightly exceeded the average, the gravitational attraction between the constituents of the matter caused the gas to coalesce into large clouds.\\nFriction inside the clouds due to collisions between the atoms and further gravitational attraction caused regions of the clouds to coalesce to densities so high as to ignite nuclear fusion, the energy source of stars.\\nUnit 3: Gravity 6 www.learner.org Figure 3: Hubble Space Telescope image of a star-forming region in the Small Magellanic Cloud.\\nSource: © NASA/ESA and A.Nota (STScI/ESA).\\nA massive star that has burnt all of its nuclear fuel can collapse under the influence of gravity into a black hole, a region of space where gravity is so strong that not even light can escape the gravitational pull.\\nNear to a black hole, therefore, nature\\'s weakest interaction exerts the strongest force in the universe.\\nHow do physicists reconcile the incredible weakness of gravity relative to the electromagnetic force with the observation that gravity dominates the interactions between the largest objects in the universe?\\nHow can it take the gravitational attraction billions of years, as calculations show, to cause two hydrogen atoms starting just 10 cm apart to collide when we know that the hydrogen gas of the early universe condensed into enormous clouds and stars on a much quicker time scale?\\nWhy does Earth\\'s gravity feel so strong while the gravitational forces between objects on the Earth are so small as to be difficult to detect?\\nThe answer, common to all of these questions, arises from the relative masses of the objects in question.\\nGravity is weak between objects that have small masses, but it grows in strength as the objects grow in mass.\\nThis seemingly simple answer reflects a profound difference between gravity and the other forces in nature.\\nAttraction without repulsion Gravity is an attractive force that acts between any objects at any distance regardless of their composition.\\nThe property of matter that gives rise to this attraction is essentially the mass of the object.\\nUnit 3: Gravity 7 www.learner.orgThe gravitational force between each atom in the Earth and each atom in our bodies is incredibly small.\\nHowever, every one of the roughly 1050 atoms in the Earth attracts each of the approximately 1027 atoms in our bodies, leading to the appreciable force that we experience.\\nIn contrast, the other forces in nature can be both attractive and repulsive.\\nThe electric force is attractive between unlike charges and equally repulsive between like charges.\\nBecause ordinary matter, such as the Earth or our bodies, consists of equal numbers of positive and negative charges bound closely together in atoms, the net electric force between electrically neutral objects essentially vanishes. Figure 4: Electrostatic and gravitational shielding.\\nSource: © Blayne Heckel.\\nIf we place an electric charge inside an otherwise empty grounded metal box, then a charge outside of the box is unaffected by the charge inside.\\nThis \"electrical shielding\" arises from the movement of charges within the metal that rearrange themselves to cancel the electric force of the charge inside.\\nIf instead, we place a mass inside a grounded metal box or any other kind of box, another mass placed outside of the box will always feel its gravitational pull, as well as the pull from the mass of the box itself.\\nBecause the gravitational force has only one sign—attractive—it cannot be shielded.\\nEvery particle, whether normal or dark matter, in regions of the early universe that had slightly higher than average density gravitationally attracted nearby particles more strongly than did regions with less than average density.\\nGravity caused the matter to coalesce into the structures in the universe that we see today.\\nAs we stand at rest, the few square inches of our feet in contact with the ground oppose the downward gravitational pull of all 1050 atoms in the Earth.\\nWhat counteracts gravity is the electrical repulsion between the outermost electrons of the soles of our shoes and the electrons at the ground\\'s surface.\\nThe mere act of standing embodies the contrast between the weak but cumulative gravitational attraction and the much stronger but self-canceling electric force.\\nUnit 3: Gravity 8 www.learner.orgSection 3: Newton\\'s Law of Universal Gravitation An underlying theme in science is the idea of unification—the attempt to explain seemingly disparate phenomena under the umbrella of a common theoretical framework.\\nThe first major unification in physics was Sir Isaac Newton\\'s realization that the same force that caused an apple to fall at the Earth\\'s surface —gravity—was also responsible for holding the Moon in orbit about the Earth.\\nThis universal force would also act between the planets and the Sun, providing a common explanation for both terrestrial and astronomical phenomena.\\nFigure 5: Newton\\'s law of universal gravitation.\\nSource: © Blayne Heckel.\\nNewton\\'s law of universal gravitation states that every two particles attract one another with a force that is proportional to the product of their masses and inversely proportional to the square of the distance between them .\\nThe proportionality constant, denoted by G, is called the universal gravitational constant .\\nWe can use it to calculate the minute size of the gravitational force inside a hydrogen atom.\\nIf we assign m1 the mass of a proton, 1.67 x 10-27 kilograms and m 2 the mass of an electron, 9.11 x 10-31 kilograms, and use 5.3 x 10-11 meters as the average separation of the proton and electron in a hydrogen atom, we find the gravitational force to be 3.6 x 10-47 Newtons.\\nThis is approximately 39 orders of magnitude smaller than the electromagnetic force that binds the electron to the proton in the hydrogen nucleus.\\nSee the math Local gravitational acceleration The law of universal gravitation describes the force between point particles.\\nYet, it also accurately describes the gravitational force between the Earth and Moon if we consider both bodies to be points with all of their masses concentrated at their centers.\\nThe fact that the gravitational force from a spherically Unit 3: Gravity 9 www.learner.orgsymmetric object acts as if all of its mass is concentrated at its center is a property of the inverse square dependence of the law of universal gravitation.\\nIf the force depended on distance in any other way, the resulting behavior would be much more complicated.\\nA related property of an inverse square law force is that the net force on a particle inside of a spherically symmetric shell vanishes.\\nFigure 6: GRACE mission gravity map of the Earth.\\nSource: © Courtesy of The University of Texas Center for Space Research (NASA/DLR Gravity Recovery and Climate Experiment).\\nJust as we define an electric field as the electric force per unit charge, we define a gravitational field as the gravitational force per unit mass.\\nThe units of a gravitational field are the same units as acceleration, meters per second squared (m/s2).\\nFor a point near the surface of the Earth, we can use Newton\\'s law of universal gravitation to find the local gravitational acceleration, g.\\nIf we plug in the mass of the Earth for one of the two masses and the radius of the Earth for the separation between the two masses, we find that g is 9.81 m/s2.\\nThis is the rate at which an object dropped near the Earth\\'s surface will accelerate under the influence of gravity.\\nIts velocity will increase by 9.8 meters per second, each second.\\nUnlike big G, the universal gravitational constant, little g is not a constant.\\nAs we move up further from the Earth\\'s surface, g decreases (by 3 parts in 105 for each 100 meters of elevation).\\nBut it also decreases as we descend down a borehole, because the mass that influences the local gravitational field is no longer that of the entire Earth but rather the total mass within the radius to which we have descended.\\nEven at constant elevation above sea level, g is not a constant.\\nThe Earth\\'s rotation flattens the globe into an oblate spheroid; the radius at the equator is nearly 20 kilometers larger than at the poles, leading Unit 3: Gravity 10 www.learner.orgto a 0.5 percent larger value for g at the poles than at the equator.\\nIrregular density distributions within the Earth also contribute to variations in g. Scientists can use maps of the gravitational field across the Earth\\'s surface to infer what structures lay below the surface.\\nGravitational fields and tides Every object in the universe creates a gravitational field that pervades the universe.\\nFor example, the gravitational acceleration at the surface of the Moon is about one-sixth of that on Earth\\'s surface.\\nThe gravitational field of the Sun at the position of the Earth is 5.9 x 10-3 m/s2, while that of the Moon at the position of the Earth is 3.3 x 10-5 m/s2, 180 times weaker than that of the Sun. Figure 7: Plot of the tidal Water Level (WL) at Port Townsend, Washington.\\nThe tides on Earth result from the gravitational pull of the Moon and Sun.\\nDespite the Sun\\'s far greater gravitational field, the lunar tide exceeds the solar tide.\\nThat\\'s because it is not the gravitational field itself that produces the tides but its gradient—the amount the field changes from Earth\\'s near side to its far side.\\nIf the Sun\\'s gravitational field were uniform across the Earth, all points on and within the Earth would feel the same force, and there would be no relative motion (or tidal bulge) between them.\\nHowever, because the gravitational field decreases as the inverse of the distance squared, the side of the Earth facing the Sun or Moon feels a larger field and the side opposite feels a smaller field than the field acting at Earth\\'s center.\\nThe result is that water (and the Earth itself to a lesser extent) bulges toward the Moon or Sun on the near side and away on the far side, leading to tides twice a day.\\nBecause the Moon is much Unit 3: Gravity 11 www.learner.orgcloser to Earth than the Sun, its gravitational gradient between the near and far sides of the Earth is more than twice as large as that of the Sun.\\nUnit 3: Gravity 12 www.learner.orgSection 4: Gravitational and Inertial Mass A subtlety arises when we compare the law of universal gravitation with Newton\\'s second law of motion.\\nThe mass that appears in the law of universal gravitation is the property of the particle that creates the gravitational force acting on the other particle; for if we double   , we double the force on   .\\nSimilarly, the mass in the law of universal gravitation is the property of the particle that responds to the gravitational force created by the other particle.\\nThe law of universal gravitation provides a definition of gravitational mass as the property of matter that creates and responds to gravitational forces.\\nNewton\\'s second law of motion, F=ma, describes how any force, gravitational or not, changes the motion of an object.\\nFor a given force, a large mass responds with a small acceleration and vice versa.\\nThe second law provides a definition of inertial mass  as the property of matter that resists changes in motion or, equivalently, as an object\\'s inertia.\\nFigure 8: Equality of gravitational and inertial mass.\\nSource: © Blayne Heckel.\\nIs the inertial mass of an object necessarily the same as its gravitational mass?\\nThis question troubled Newton and many others since his time.\\nExperiments are consistent with the premise that inertial and gravitational mass are the same.\\nWe can measure the weight of an object by suspending it from a spring balance.\\nEarth\\'s gravity pulls the object down with a force (weight) of   , where g is the local gravitational acceleration and    the gravitational mass of the object.\\nGravity\\'s pull on the object is Unit 3: Gravity 13 www.learner.orgbalanced by the upward force provided by the stretched spring.\\nWe say that two masses that stretch identical springs by identical amounts have the same gravitational mass, even if they possess different sizes, shapes, or compositions.\\nBut will they have the same inertial mass?\\nWe can answer this question by cutting the springs, letting the masses fall, and measuring the accelerations.\\nThe second law says the net force acting on the mass is the product of the inertial mass,   , and acceleration, a, giving us:  or   .\\nBut g is a property of the Earth alone and does not depend upon which object is placed at its surface, while experiments find the acceleration, a, to be the same for all objects falling from the same point in the absence of air friction.\\nTherefore,    is the same for all objects and thus for .\\nWe define the universal gravitational constant, G, to make   .\\nThe principle of the universality of free fall  is the statement that all materials fall at the same rate in a uniform gravitational field.\\nThis principle is equivalent to the statement that   .\\nPhysicists have found the principle to be valid within the limits of their experiments\\' precision, allowing them to use the same mass in both the law of universal gravitation and Newton\\'s second law.\\nMeasuring G Measurements of planets\\' orbits about the Sun provide a value for the product GM s, where M S is the mass of the Sun.\\nSimilarly, earthbound satellites and the Moon\\'s orbit provide a value for GM E, where ME is the mass of the Earth.\\nTo determine a value for G alone requires an a priori knowledge of both masses involved in the gravitational attraction.\\nPhysicists have made the most precise laboratory measurements of G using an instrument called a \"torsion balance,\" or torsion pendulum .\\nThis consists of a mass distribution suspended by a long thin fiber.\\nUnbalanced forces that act on the suspended mass distribution can rotate the mass distribution; the reflection of a light beam from a mirror attached to the pendulum measures the twist angle.\\nBecause a very weak force can twist a long thin fiber, even the tiny torques created by gravitational forces lead to measurable twist angles.\\nUnit 3: Gravity 14 www.learner.org Figure 9: Schematic of a torsion balance to measure the gravitational constant, G. Source: © Blayne Heckel.\\nTo measure G, physicists use a dumbbell-shaped mass distribution (or more recently a rectangular plate) suspended by the fiber, all enclosed within a vacuum vessel.\\nPrecisely weighed and positioned massive spheres are placed on a turntable that surrounds the vacuum vessel.\\nRotating the turntable with the outer spheres about the fiber axis modulates the gravitational torque that the spheres exert on the pendulum and changes the fiber\\'s twist angle.\\nThis type of experiment accounts in large part for the currently accepted value of (6.67428  0.00067)\\nx 10-11 N-m2/kg2 for the universal gravitational constant.\\nIt is the least precisely known of the fundamental constants because the weakness of gravity requires the use of relatively large masses, whose homogeneities and positioning are challenging to determine with high precision.\\nDividing GM E found from satellite and lunar orbits by the laboratory value for G allows us to deduce the mass of the Earth: 5.98 X 1024 kilograms.\\nUnit 3: Gravity 15 www.learner.orgSection 5: Testing the Law of Universal Gravitation Early successes of the law of universal gravitation included an explanation for Kepler\\'s laws of planetary orbits and the discovery of the planet Neptune.\\nLike any physical law, however, its validity rests on its agreement with experimental observations.\\nAlthough the theory of general relativity has replaced the law of universal gravitation as our best theory of gravity, the three elements of the universal law —the universal constant of gravitation, the equality of gravitational and inertial mass, and the inverse square law—are also key elements of general relativity .\\nTo test our understanding of gravity, physicists continue to examine these elements of the universal law of gravitation with ever-increasing experimental sensitivity.\\nFigure 10: Mass of a helium atom is not equal to the sum of the individual masses of its constituent parts.\\nSource: The mass of an object does not equal the sum of the masses of its constituents.\\nFor example, the mass of a helium atom is about one part per thousand less than the sum of the masses of the two neutrons, two protons, and two electrons that comprise it.\\nThe mass of the Earth is about five parts in 1010 smaller than the sum of the masses of the atoms that make up our planet.\\nThis difference arises from the nuclear and electrostatic binding—or potential—energy that holds the helium atom together and the gravitational binding (potential) energy that holds the earth together.\\nThe inertial mass of an object therefore has contributions from the masses of the constituents and from all forms of binding energy that act within the object.\\nIf   , gravity must act equally on the constituent masses and the nuclear, electrostatic, and gravitational binding energies.\\nIs this indeed the case?\\nDoes Unit 3: Gravity 16\\nwww.learner.orgthe Sun\\'s gravity act on both the atoms in the Earth and the gravitational binding energy that holds the Earth together?\\nThese are questions that have to be answered by experimental measurements.\\nModern tests of the universality of free fall tell us that the answer to these questions is yes, at least to within the precision that the measurements have achieved to date.\\nTests of the universality of free fall To test the universality of free fall (UFF), experimentalists compare the accelerations of different materials under the influence of the gravitational force of a third body, called the \"source.\"\\nMany of the most sensitive tests have come from torsion balance measurements.\\nA recent experiment used eight barrel- shaped test bodies attached to a central frame, with four made of beryllium (Be) on one side and four of titanium (Ti) on the other.\\nThe denser titanium bodies were hollowed out to make their masses equal to those of the beryllium bodies while preserving the same outer dimensions.\\nAll surfaces on the pendulum were coated by a thin layer of gold.\\nThe vacuum vessel that surrounded and supported the torsion fiber and pendulum rotated at a slow uniform rate about the tungsten fiber axis.\\nAny differential acceleration of the two types of test bodies toward an external source would have led to a twist about the fiber that changed in sign as the apparatus rotated through 180°.\\nEssential to the experiment was the removal of all extraneous (nongravitational) forces acting on the test bodies.\\nFigure 11: Torsion pendulum to test the universality of free fall.\\nSource: © Blayne Heckel.\\nUnit 3: Gravity 17 www.learner.orgFor source masses, experiments have used locally constructed masses within the laboratory, local topographic features such as a hillside, the Earth itself, the Sun, and the entire Milky Way galaxy.\\nComparing the differential acceleration of test bodies toward the galactic center is of particular interest.\\nTheorists think that dark matter causes roughly 30 percent of our solar system\\'s acceleration about the center of the galaxy.\\nThe same dark matter force that helps to hold the solar system in orbit about the galactic center acts on the test bodies of a torsion pendulum.\\nA dark matter force that acts differently on different materials would then lead to an apparent breakdown of the UFF.\\nBecause physicists have observed no differential acceleration in the direction of the galactic center, they conclude that dark matter interacts with ordinary matter primarily through gravity.\\nNo violation of the UFF has yet been observed.\\nPhysicists use tests of the UFF to search for very weak new forces that may act between objects.\\nSuch forces would lead to an apparent violation of the UFF and would be associated with length scales over which the new forces act.\\nDifferent experimental techniques have been used to test the UFF (and search for new forces) at different length scales.\\nFor example, there is a region between 103 meters and 105 meters over which torsion balances fail to produce reliable constraints on new weak forces.\\nThis is because over this length scale, we do not have sufficient knowledge of the density homogeneity of the Earth to calculate reliably the direction of the new force— it might point directly parallel to the fiber axis and not produce a torque on the pendulum.\\nIn this length range, the best limits on new forces come from modern \"drop tower\" experiments that directly compare the accelerations of different materials in free fall at the Earth\\'s surface.\\nUFF tests in space The future for tests of the UFF may lie in space-based measurements.\\nIn a drag-free satellite, concentric cylinders of different composition can be placed in free fall in the Earth\\'s gravitational field.\\nExperimentalists can monitor the relative displacement (and acceleration) of the two cylinders with exquisite accuracy for long periods of time using optical or superconducting sensors.\\nSatellite-based measurements might achieve a factor of 1,000 times greater sensitivity to UFF violation than ground- based tests.\\nUnit 3: Gravity 18 www.learner.org Figure 12: Apollo mission astronauts deploy corner cube reflectors.\\nOne source of space-based tests of the UFF already exists.\\nThe Apollo space missions left optical corner mirror reflectors on the Moon that can reflect Earth-based laser light.\\nAccurate measurements of the time of flight of a laser pulse to the Moon and back provide a record of the Earth-Moon separation to a precision that now approaches 1 millimeter.\\nBecause both the Earth and the Moon are falling in the gravitational field of the Sun, this lunar laser ranging (LLR) experiment provides a test of the relative accelerations of the Earth and Moon toward the Sun with precision of 2 x 10-13 of their average accelerations.\\nGravitational binding energy provides a larger fraction of the Earth\\'s mass than it does for the Moon.\\nWere the UFF to be violated because gravity acts differently on gravitational binding energy than other types of mass or binding energy, then one would expect a result about 2,000 times larger than the experimental limit from LLR.\\nValidating the inverse square law Physicists have good reason to question the validity of the inverse square law at both large and short distances.\\nShort length scales are the domain of the quantum world, where particles become waves and we can no longer consider point particles at rest.\\nFinding a theory that incorporates gravity within quantum mechanics has given theoretical physicists a daunting challenge for almost a century; it remains an open question.\\nAt astronomical length scales, discrepancies between observations and the Unit 3: Gravity 19 www.learner.orgexpectations of ordinary gravity require dark matter and dark energy to be the dominant constituents of the universe.\\nHow sure are we that the inverse square law holds at such vast distances? Figure 13: Torsion pendulum to test the inverse square law of gravity at submillimeter distances.\\nSource: © Blayne Heckel.\\nThe inverse square law has been tested over length scales ranging from 5 x 10-5 to 1015 meters.\\nFor the large lengths, scientists monitor the orbits of the planets, Moon, and spacecraft with high accuracy and compare them with the orbits calculated for a gravitational force that obeys the inverse square law (including small effects introduced by the theory of general relativity).\\nAdding an additional force can lead to measurable modifications of the orbits.\\nFor example, general relativity predicts that the line connecting the perihelia and aphelia of an elliptical gravitational orbit (the points of closest and furthest approach to the Sun for planetary orbits, respectively) should precess slowly.\\nAny violation of the inverse square law would change the precession  rate of the ellipse\\'s semi-major axis.\\nSo far, no discrepancy has been found between the observed and calculated orbits, allowing scientists to place tight limits on deviations of the inverse square law over solar system length scales.\\nUnit 3: Gravity 20 www.learner.org Figure 14: Experimental limits on the universality of free fall.\\nSource: © Blayne Heckel.\\nAt the shortest distances, researchers measure the gravitational force between plates separated by about 5 x 10-5 meters, a distance smaller than the diameter of a human hair.\\nA thin conducting foil stretched between the plates eliminates any stray electrical forces.\\nRecent studies using a torsion pendulum have confirmed the inverse square law at submillimeter distances.\\nTo probe even shorter distances, scientists have etched miniature (micro) cantilevers and torsion oscillators from silicon wafers.\\nThese devices have measured forces between macroscopic objects as close as 10-8 meters, but not yet with enough sensitivity to isolate the gravitational force.\\nDoes the inverse square law hold at the tiny distances of the quantum world and at the large distances where dark matter and dark energy dominate?\\nWe don\\'t know the answer to that question.\\nDefinitive tests of gravity at very small and large length scales are difficult to perform.\\nScientists have made progress in recent years, but they still have much to learn.\\nUnit 3: Gravity 21 www.learner.orgSection 6: The Theory of General Relativity\\nThe law of universal gravitation describes a force that acts instantaneously between objects separated by arbitrarily large distances.\\nThis behavior is in conflict with the theory of special relativity , which forbids the transfer of information faster than the speed of light.\\nSo how does one construct a theory of gravity that is consistent with special relativity?\\nEinstein found the key: the apparent equivalence between gravity and acceleration.\\nImagine that you are in a windowless rocket ship far from any stars or planets.\\nWith the rocket engines turned off, you and everything else not secured to the rocket float freely in weightlessness within the rocket\\'s cabin.\\nWhen turned on, the rocket engines provide a constant acceleration—9.8 m/s2, say—and you stand firmly on the floor directly above the engines.\\nIn fact, the floor pushes against your feet with the same force that the ground pushes against your feet when you stand on Earth.\\nEinstein posed the question: Is there any experiment you could perform within your sealed rocket ship that could distinguish between being in a rocket with a constant acceleration of 9.8 m/s2, or a rocket at rest on the launch pad on Earth? Figure 15: A person in an accelerating rocket feels the same downward pull as a person on Earth feels from gravity.\\nSource: © M. Poessel/Einstein Online/Max Planck Institute for Gravitational Physics.\\nEinstein concluded that the answer was no: There is no way to tell the difference between the presence of a uniform gravitational field and a frame of reference that has a constant acceleration.\\nThis observation embodies Einstein\\'s principle of equivalence, the equivalence of gravity and acceleration, on which he built the theory of general relativity.\\nGravitational lensing and the bending of light Unit 3: Gravity 22 www.learner.orgWe can use the equivalence between an accelerated reference frame and a frame with a uniform gravitational field to infer the behavior of light in a gravitational field.\\nImagine a beam of light traveling horizontally from one side of the rocket cabin to the other.\\nWith the rocket engines off, the light follows a straight path across the cabin in accordance with the laws of special relativity.\\nWith the engines on, causing constant acceleration, the cabin moves slightly upward in the time it takes the light to travel across the cabin.\\nHence, the light beam strikes a point lower on the cabin wall than when the engines were off.\\nIn the frame of the accelerating rocket, the light beam follows a curved (parabolic) path.\\nBecause an accelerating rocket is equivalent to a rocket at rest in a uniform gravitational field, a light beam will follow a curved path in a gravitational field; in other words, light is bent by gravity.\\nA famous observation during a solar eclipse in 1919 confirmed that prediction: Measurements showed that starlight passing near the edge of the eclipsed Sun was deflected by an amount consistent with the principle of equivalence.\\nFigure 16: Bending of light in an accelerating rocket.\\nSource: © Blayne Heckel.\\nIn the absence of gravity, a distant galaxy will appear to an observer on Earth to be a tiny source of light.\\nHowever, if there are mass distributions such as other galaxies or clouds of dark matter near to the line sight between the Earth and the distant light source, the gravity from these mass distributions will bend the light from the distant galaxy.\\nThe image of the distant galaxy on Earth can then become a ring, one or multiple arcs, or even appear as several galaxies depending upon the location and distribution of the intervening mass.\\nThis distortion of light from distant sources is called gravitational lensing  and is well established in observations from modern telescopes.\\nThe observed gravitational lensing is used to infer what sources of gravity lie between the Earth and distant light sources.\\nA related phenomenon is an Unit 3: Gravity 23 www.learner.orgincrease in intensity of the light observed from a distant source due to the passage of a massive object near to the line of sight.\\nThe gravitational field of the moving object acts as a lens, focusing more light into the telescope during the time that the massive object is near to the line of sight.\\nGravitational time dilation Returning to our rocket ship thought-experiment, imagine that a light beam travels from the ceiling to the floor of the accelerating rocket.\\nIn the time the beam takes to traverse the cabin, the cabin floor has acquired a larger velocity than it had when the light left the ceiling.\\nA device on the floor measuring the frequency of the light would find a higher frequency than that of the emitted beam because of the Doppler shift , a phenomenon noticed most commonly in an ambulance siren that has a higher pitch as the ambulance approaches and a lower pitch as it recedes.\\nThe principle of equivalence then asserts that, in a gravitational field, a light beam traveling opposite to the field acquires a higher frequency, shifted toward the blue end of the spectrum; while a light beam shining upward from the Earth\\'s surface decreases in frequency as it rises, the effect that we know as the gravitational redshift.\\nAgain, experiments have confirmed this phenomenon.\\nFigure 17: Time dilation and the twin paradox.\\nSource: © Blayne Heckel.\\nAn inertial (nonaccelerating) observer sees no change in the light\\'s frequency—the frequency associated with the atomic transition generating the light—as the light moves across the cabin, because it is traveling freely through empty space.\\nYet, an observer on the rocket floor, accelerating with the rocket, can use the same, now accelerating, atoms and atomic transitions as a clock (see Unit 5 for details); the observer defines a second as the time required for the fixed number of oscillations of a specific atomic transition.\\nWe concluded in the last paragraph that this accelerating observer will see the frequency of the light beam to be higher than the frequency of the same atomic transition in the measuring device.\\nThe Unit 3: Gravity 24 www.learner.orginescapable conclusion is that the atomic clock (like all clocks) ticks more slowly in the accelerating frame of reference.\\nBy the principle of equivalence, clocks in a gravitational field tick more slowly than in the absence of the field; the stronger the field, the more slowly the clock ticks.\\nAn atomic clock at sea level loses five microseconds per year relative to an identical clock at an altitude of 5,000 feet.\\nWe age more slowly at sea level than on a mountaintop.\\nThe global positioning system (GPS) relies heavily on the accuracy of clocks and corrects for the gravitational time dilation  to achieve its fantastic precision.\\nCurved spacetime The second key ingredient of general relativity is the notion of curved spacetime .\\nSpecial relativity combines space and time into a four-dimensional spacetime, often referred to as \"flat spacetime\" or \"Minkowski space.\"\\nIn flat spacetime, Euclidean geometry describes the spatial dimensions: Parallel lines never intersect, and the sum of the interior angles of a triangle is always 180 degrees.\\nThe two- dimensional analogue of flat space is the Cartesian plane, familiar from high school geometry class.\\nThe surface of a sphere is also a two dimensional surface, but one that must be described by non- Euclidean geometry.\\nLines of constant longitude are parallel at the equator yet intersect at the poles.\\nIf you start at the equator and walk due north to the pole, turn right by 90 degrees, walk south to the equator, and then turn right again and walk along the equator, you will return to your original position having taken a triangular path on the Earth\\'s surface.\\nThe sum of the interior angles of your triangular path is 270 degrees.\\nA spherical surface is said to have positive curvature; a saddle-shaped surface has a negative curvature; and the sum of the interior angles of a triangle drawn on a saddle is less than 180 degrees.\\nUnit 3: Gravity 25 www.learner.org Figure 18: Triangles on curved surfaces.\\nSource: © Blayne Heckel.\\nViewed from three dimensions, the shortest path between two points on a spherical or saddle-shaped surface is a curved line.\\nThis \"geodesic\" is the path that light would follow in that space.\\nThree- dimensional space and four-dimensional spacetime, described by Riemannian geometry, can also be curved.\\nThe geodesics in spacetime are the paths that light beams follow—or, equivalently, the paths that observers in free fall follow.\\nThe Earth is in free fall about the Sun.\\nWe can construct a curved spacetime in which a circular orbit about the Sun is a geodesic.\\nIn such a spacetime, the Earth\\'s orbit would stem from the curvature of spacetime rather than from a force acting between the Earth and the Sun.\\nThe theory of general relativity takes the equivalence between motion in a gravitational field and motion in curved spacetime one step further.\\nIt asserts that what we call gravity is the bending of spacetime by matter.\\nRather than viewing gravity as a force acting between objects in flat spacetime, we should understand gravity as the interaction between matter and spacetime.\\nThe field equations of general relativity specify how matter and energy determine the curvature of spacetime.\\nIn turn, the spacetime curvature determines how the matter and energy will evolve.\\nBlack holes If enough matter and energy are concentrated in a small enough volume, general relativity predicts that spacetime can become so highly curved that a black hole  is formed.\\nA black hole is characterized by an event horizon , a surface surrounding the enclosed matter, through which nothing can escape; the event Unit 3: Gravity 26 www.learner.orghorizon represents a surface of no return.\\nTo an outside observer, the black hole is completely described by just three numbers: its mass, its electric charge, and its angular momentum.\\nFigure 19: As gas falls into this supermassive black hole, it emits x- rays.\\nSource: © NASA, ESA, A.M. Koekemoer (STScI), M. Dickinson (NOAO), and the GOODS team.\\nBlack holes can be created when a star of sufficient mass, after having burnt its nuclear fuel, collapses under its own weight.\\nThe black hole grows by capturing nearby matter and radiation that is pulled through the event horizon and by merging with astronomical objects such as stars, neutron stars, and other black holes.\\nMassive black holes, millions to billions times more massive than our Sun, have been found near the center of many galaxies, including our own Milky Way.\\nThe black holes become visible when they accrete gas from the surrounding regions; the gas is accelerated and heated, producing observable radiation, before falling through the event horizon.\\nThe presence of a black hole can also be inferred from its gravitational influence on the orbits of nearby stars.\\nUnit 3: Gravity 27 www.learner.orgSection 7: Gravitational Waves Gravitational waves are the gravitational analogues to electromagnetic waves—electric and magnetic fields that oscillate in the plane perpendicular to the direction that the wave travels.\\nSimilarly, gravitational waves are gravitational fields that oscillate perpendicular to the direction of travel.\\nUnlike electromagnetic waves, which can be produced by a single oscillating electric charge, conservation of linear momentum requires at least two masses moving in opposition to produce gravitational waves.\\nIn the theory of special relativity, the constant c, called the speed of light, connects space with time and is the speed at which all massless particles travel.\\nLike electromagnetic waves, gravitational waves are believed to propagate at the speed c. General relativity predicts the existence of gravitational waves.\\nIn matter-free regions of spacetime where gravity is weak, the field equations of general relativity simplify to wave equations for spacetime itself.\\nThe solutions to these equations are transverse ripples in spacetime, propagating at the speed of light, which we identify as gravitational waves.\\nThe distortion of spacetime caused by a gravitational wave is distinctive: In the plane perpendicular to the direction the wave is travelling, space is stretched along one axis and compressed along the orthogonal axis, and vice versa one half-wave cycle later.\\nFigure 20: Distortion of space from a gravitational wave.\\nSource: © Blayne Heckel.\\nWhat are the similarities and differences between electromagnetic and gravitational waves?\\nBoth waves travel at speed c and carry with them energy and momentum.\\nFor electromagnetic waves, spacetime Unit 3: Gravity 28 www.learner.orgis the background medium in which the waves travel, while for gravitational waves, spacetime itself constitutes the waves.\\nElectromagnetic waves are produced by accelerating or oscillating electric charges, while gravitational waves are produced by accelerating or oscillating mass distributions.\\nThe frequencies of both waves reflect the oscillation frequencies of the sources that produce them.\\nElectronic, vibrational, and rotational transitions (that is, oscillations) in atoms and molecules provide the most common source of electromagnetic waves, producing wave frequencies between roughly 107 and 1017 Hertz (Hz, or cycles per second).\\nThe most efficient sources for gravitational waves are massive objects undergoing rapid acceleration, such as pairs of neutron stars and/or black holes orbiting closely about one another.\\nConsiderations of orbital speeds and masses lead us to expect that the strongest gravitational radiation will have frequencies less than 10,000 Hz.\\nElectromagnetic waves interact strongly with matter through absorption or scattering.\\nGravitational waves, by contrast, interact extremely weakly with matter; they travel essentially unimpeded through spacetime.\\nIndirect detection of gravitational waves The most obvious difference between gravitational and electromagnetic waves is the fact that no one has yet directly detected gravitational waves—although this situation should change soon, given the significant progress in the technologies necessary for detection.\\nIn the meantime, we have strong indirect evidence that gravitational radiation exists.\\nAstronomers have monitored the orbital frequency of the binary neutron star system PSR1913+16 since 1974, the year that Russell Hulse and Joseph Taylor discovered the system.\\nOne of the neutron stars is a pulsar that beams radio waves to the Earth as the neutron star rotates about its axis.\\nAstrophysicists use the arrival times of the radio pulses to reconstruct the orbit of the binary system.\\nThe oscillating mass distribution of this binary system should generate gravitational waves and lose orbital energy as the waves radiate outward.\\nA loss in orbital energy moves the neutron stars closer together and decreases the orbital period.\\nThe observed decrease of the orbital period over the past 35 years agrees with the energy loss through gravitational radiation predicted by general relativity to better than 1 percent accuracy.\\nUnit 3: Gravity 29 www.learner.org Figure 21: Orbital period of the binary neutron star system PSR 1913+15 measured from 1975 to 2000.\\nSource: © Wikimedia Commons, Public Domain.\\nAuthor: M. Haynes et Lorimer, 2001.\\nRadio pulses from pulsars arrive at such a regular rate as to provide hope that pulsars may provide a means to detect very low frequency gravitational waves.\\nWaves with frequencies around 10-9 Hz (equivalent to wavelengths of around 10 light-years) may persist from mass motions early in the history of the universe.\\nWhen such a wave passes a pulsar, it slightly alters the arrival time of the radio beam from the pulsar.\\nBy comparing the arrival times of signals from perhaps 100 pulsars spread across the sky for many years, astronomers might possibly detect the tell-tale distortion of spacetime that is the signature of a passing gravitational wave.\\nResearchers believe that even lower frequency (that is, longer wavelength) gravitational waves were created in the early moments of the universe.\\nWe have evidence for events around 380,000 years after the Big Bang in the form of extraordinarily precise measurements of the cosmic microwave background (CMB), which is electromagnetic radiation left over from the early universe.\\nPrimordial gravitational waves would leave their imprint on the CMB as a distinctive polarization  pattern as one compares the polarization of CMB radiation from different regions across the sky.\\nIntense efforts are under way to mount instruments with enough polarization sensitivity to search for the primordial gravitational waves.\\nBoth ground-based observations (CLOVER, EBEX, Polarbear, QUIET, SPIDER, and SPUD instruments, Unit 3: Gravity 30 www.learner.orgto name a few) and space-based measurements from the Planck satellite launched in 2009 promise rapid progress toward the detection of primordial gravitational waves.\\nDirect detection of gravitational waves Unit 3: Gravity\\n31 www.learner.orgThe Classic Michelson Interferometer Source: © Blayne Heckel.\\nOriginally devised as part of the fruitless 19th century effort to identify the \"ether\" that supposedly suffused space, the Michelson interferometer now finds application in a 21st century experiment: the search for gravitational waves.\\nThe diagram shows the original version of the instrument.\\nA beam splitter divides laser light entering the input port into a transmitted beam and a reflected beam, perpendicular to each other.\\nAt the end of each beam\\'s path, a mirror reflects the light back toward the beam splitter.\\nIf the two beams\\' paths have exactly the same length, the beams\\' electric fields oscillate in phase when the light returns to the beam splitter.\\nThe beams recombine to produce a beam that exits the beam splitter along the output port.\\nIf the two paths differ in length by half a wavelength, they are out of phase.\\nIn that case, they interfere destructively at the beam splitter and no light exits from the output port.\\nThe intensity of light leaving the output port changes from a maximum to zero as the relative distance to the end mirrors changes by a quarter of a wavelength—about 2.5 x 10-7 meters for typical laser light.\\nPrecisely measuring this light intensity allows experimenters to detect even smaller relative displacements of the mirrors.\\nAny passing gravitational wave should compress spacetime in one direction and stretch it out in the perpendicular direction.\\nPhysicists believe that a modern version of the Michelson interferometer has the precise measuring ability that can detect the difference between the two.\\nUnit 3: Gravity 32 www.learner.orgThe earliest attempts to detect gravitational waves directly used resonant mass detectors, also called \"bar detectors,\" first developed by Joseph Weber.\\nA typical bar detector might be a 5000 kg cylinder, two meters long, suspended in vacuum, and made from a low mechanical loss material such as certain alloys of aluminum.\\nA burst of gravitational radiation could stretch and compress the bar, exciting the roughly one kilohertz lowest frequency vibrational mode of the cylinder.\\nSensors at the ends of the cylinder would detect the vibrations.\\nA low-loss material would ring for many vibrational cycles, enhancing the ability to identify the excess vibration from a gravitational wave in the presence of background noise.\\nModern versions of the bar detectors (for example, the NAUTILUS and AURIGA detectors in Italy, miniGRAIL in the Netherlands, and the EXPLORER bar in Switzerland) are cooled to liquid helium temperatures or even lower to reduce the mechanical losses and thermal vibrations, and to reduce the noise inherent in the motion sensors.\\nFigure 22: Nautilus cryogenic antenna at the Laboratori Nazionali di Frascati, Italy.\\nSource: © Italian National Institute of Nuclear Physics (INFN)— National Laboratory of Frascati.\\nThe most developed technology for the detection of gravitational waves involves long baseline laser interferometers.\\nThese instruments use laser light as a \"meter stick\" to compare the distances between a central object and distant objects along perpendicular axes.\\nA passing gravitational wave will compress spacetime along one axis while stretching it along a perpendicular axis.\\nAn interferometer provides a precise measurement of the relative distance that light travels along different paths.\\nThe long baseline gravitational wave interferometers are refined versions of the Michelson interferometer that, when it failed to detect the ether late in the 19th century, helped to set the scene for the theory of special relativity.\\nBut instead of being mounted rigidly on a table, the end mirrors of the gravitational wave instruments are suspended, like pendulums, from thin wires.\\nIn addition, the entire laser path occurs Unit 3: Gravity 33 www.learner.orgwithin a vacuum chamber.\\nIn the horizontal plane, the end mirrors are essentially objects in freefall, able to follow the stretching and compressing of spacetime from a gravitational wave.\\n(In the classical picture of gravitational waves, the waves produce horizontal forces on the end mirrors; suspended mirrors can move in response to the wave forces.)\\nHowever, even the strongest gravitational waves that one might hope to detect on Earth stretch space by an extremely small amount: The strain (change in distance divided by the distance) between two objects is expected to be less than 10-18.\\nTo make the change in distance large enough for an interferometer to detect, designers must make the baseline as long as possible.\\nGravitational wave discovery on Earth and in space Figure 23: Aerial view of the LIGO Observatory at Hanford, Washington.\\nSource: © LIGO Laboratory.\\nThe LIGO (Laser Interferometer Gravitational Wave Observatory) interferometers in the states of Louisiana and Washington each have end mirrors 4 kilometers from the beam splitter.\\nVIRGO in Italy, GEO in Germany, and TAMA in Japan have separation distances of 3 kilometers, 600 meters, and 300 meters, respectively.\\nWith the 4-kilometer separation, physicists expect a strong gravitational wave to produce a relative change in distance between the mirrors and beam splitter of only about 4 x 10-15 meters, roughly the size of an atomic nucleus.\\nHaving several gravitational wave interferometers operating simultaneously greatly improves the chances of distinguishing a gravitational wave from the inevitable background sources of noise.\\nGround-based gravitational wave interferometers are designed to detect waves with frequencies between roughly 10 Hz and 1,000 Hz.\\nSources for gravitational waves in this frequency band include the final moments of the in-spiral of orbiting pairs of neutron stars or black holes that lead to their collision and Unit 3: Gravity 34 www.learner.orgmerger into a single object, violent astronomical events such as supernovae, and constant frequency signals such as those from a rapidly rotating neutron star that has a residual mass quadrupole moment. Figure 24: Artist\\'s conception of the LISA satellites in space.\\nGround motion and seismic noise increase rapidly below a frequency of about 10 Hz and prevent Earth- based interferometers from detecting gravitational waves below this frequency limit.\\nHowever, placing the interferometer on satellites in space allows us to avoid seismic noise and to envision much larger separations between the components of the interferometer.\\nLISA (Laser Interferometer Space Antenna) is a joint NASA/European Space Agency proposal to launch three satellites into orbits to form an equilateral triangle with a distance of 5 x 106 kilometers between each spacecraft.\\nLaser light exchanged between the spacecraft will measure the relative distances between them and may detect gravitational waves within a frequency range of 10-4 Hz to 0.1 Hz.\\nSources for gravitational waves in this frequency band include massive black hole binaries that form after galactic mergers, the orbits of stars as they spiral into black holes, and the gravitational radiation from the orbits of millions of compact binary systems within our Unit 3: Gravity 35 www.learner.orgMilky Way galaxy.\\nOnce the detection of gravitational waves becomes routine, a new field of gravitational wave astronomy will be born.\\nUnit 3: Gravity 36 www.learner.orgSection 8: Gravity and Quantum Mechanics Figure 25: Visualization of a stage in the quantum evolution of geometry, according to Loop Quantum Gravity.\\n© T. Thiemann (Max Planck Institute for Gravitational Physics (Albert Einstein Institute)) & Mildemarketing Science Communication.\\nDespite the fact that there is no experimental evidence that conflicts with the predictions of general relativity, physicists have found compelling reasons to suspect that general relativity may be only a good approximation to a more fundamental theory of gravity.\\nThe central issue is reconciling general relativity with the demands of quantum mechanics.\\nWell tested by experiment, quantum mechanics is the theory that describes the microscopic behavior of particles.\\nUnit 5 of this course will delve into the details of quantum mechanics.\\nIn the quantum world, particles are also waves, the results of measurements are probabilistic in nature, and an uncertainty principle forbids knowing certain pairs of measurable quantities, such as position and momentum, to arbitrary precision.\\nThe Standard Model described in the previous two units provides a unified picture of the strong, weak, and electromagnetic forces within the framework of quantum mechanics.\\nNonetheless, theoretical physicists have found it to be extremely difficult to construct a theory of quantum gravity that incorporates both general relativity and quantum mechanics.\\nAt the atomic scale, gravity is some 40 orders of magnitude weaker than the other forces in nature.\\nIn both general relativity and Newtonian gravity, the strength of gravity grows at shorter and shorter distances, while quantum effects prevent the other forces from similarly increasing in strength.\\nAt a distance of approximately 10-35 m, called the Planck length, gravity becomes as strong as the other forces.\\nAt the Planck length, gravity is so strong and spacetime is so highly distorted that our common notions of space and time lose meaning.\\nQuantum fluctuations at this length scale produce energies so Unit 3: Gravity 37 www.learner.orglarge that microscopic black holes would pop into and out of existence.\\nA theory of quantum gravity is needed to provide a description of nature at the Planck length.\\nYet, attempts by researchers to construct such a theory, analogous to the Standard Model of particle physics, have lead to serious inconsistencies.\\nTheories of quantum gravity A significant difference between a quantum theory of gravity and the Standard Model of particle physics is the role of spacetime in the theory.\\nIn the Standard Model, spacetime is a background in which the quantum particles interact.\\nIn quantum gravity, spacetime itself participates in the interactions and acquires quantum fluctuations.\\nTheorists have proposed radically new ideas about spacetime at microscopic distances to serve as foundations for theories of quantum gravity.\\nLoop Quantum Gravity is an approach in which spacetime itself arises from the theory as a grid of discrete (quantized) loops of gravitational field lines called \"spin networks.\"\\nIn Causal Dynamical Triangulation, spacetime is two- dimensional at the Planck length scale and evolves into our four-dimensional spacetime at larger length scales.\\nFigure 26: Causal Dynamical Triangulation builds the spacetime in which we live from tiny triangles.\\nSource: © Paul Coddington, University of Adelaide.\\nThe most studied candidate for a theory of quantum gravity, string theory, posits that elementary particles are not points in spacetime but rather one-dimensional objects like open lengths or closed loops of string.\\nDifferent modes of vibrations of the elementary strings give rise to the spectrum of particles in nature including the graviton, the particle that carries the gravitational force (analogous to the photon in electromagnetism).\\nTo provide a realistic theory of quantum gravity, string theories require extra spatial Unit 3: Gravity 38 www.learner.orgdimensions, each normally viewed as being finite in extent, such as a one-dimensional circle with a radius of the Planck length or larger.\\nThe presence of extra dimensions and new particles associated with gravity in string theories alters the gravitational inverse square law and the equivalence principle at very short distances.\\nWe will learn more about string theory and extra dimensions in Unit 4.\\nThe small length scales and equivalently high energy scales at which quantum effects should modify gravity are far beyond the reach of current experimental techniques.\\nA major challenge to finding the correct theory of quantum gravity is that it will be difficult to find experimental evidence to point us in the right direction.\\nGravity at large distances We can also wonder how well we know the behavior of gravity at very large lengths scales.\\nAs we have seen, the inverse square law of gravity has been verified over solar system distances, but the observable universe is 100 billion times larger than that.\\nIt requires a leap of faith to believe that our local laws of gravity hold everywhere.\\nSome of the evidence for dark matter relies upon comparing the observed acceleration of objects far apart to that expected from the inverse square law.\\nIf the law of universal gravity is invalid for very small accelerations, as proposed in the MOND ( Modified Newtonian Dynamics ) theory, then our expectations for the interactions of distant objects would change.\\nFigure 27: Simulations of structure formation in the universe show the influence of gravity and dark energy.\\nSource: © Raul Angulo, Max Planck Institute for Astrophysics.\\nUnit 3: Gravity 39 www.learner.orgDark energy, described in detail in Unit 11, has been proposed to explain why the expansion rate of the universe appears to be accelerating.\\nThe evidence for dark energy rests upon the comparison of observations with the predictions of general relativity applied to very large length scales.\\nTheorists continue to explore a variety of ways to modify general relativity to circumvent the need for dark energy.\\nAs there is no direct experimental evidence one way or another, the behavior of gravity and very large length scales is still an open question.\\nThe first unification in physics was Newton\\'s law of universal gravitation that provided a common explanation for the motion of terrestrial and heavenly objects.\\nIt is ironic that for modern attempts to unify all of the forces in nature, gravity is the last and most difficult force to include.\\nThe theory of general relativity was a triumph of 20th century physics that revolutionized our concepts of space and time.\\nYet, even general relativity is not likely to be the ultimate theory of gravity.\\nThere is still much to be learned about gravity.\\nUnit 3: Gravity 40 www.learner.orgSection 9: Further Reading •Avery Broderick and Abraham Loeb, \"Portrait of a Black Hole,\" Scientific American , December 2009, p. 42.\\n•George Gamov, \"Gravity,\" Dover Publications, Inc., 2002.\\n•GRACE Mission website: http://www.csr.utexas.edu/grace/. •Eduardo Gueron, \"Adventures in Curved Spacetime,\" Scientific American , August 2009, p. 38.\\n•Pankaj S. Joshi, \"Naked Singularities,\" Scientific American , February 2009, p. 36.\\n•Jerzy Jurkiewicz, Renate Loll, and Jan Ambjorn, \"The Self-Organizing Quantum Universe,\" Scientific American , July 2008, p. 42.\\n•Laser Interferometer Gravitational Wave Observatory (LIGO) website: http://www.ligo.caltech.edu/. Unit 3: Gravity 41 www.learner.orgGlossary black hole : A black hole is a region of space where gravity is so strong that nothing can escape its pull.\\nBlack holes have been detected through their gravitational influence on nearby stars and through observations of hot gas from surrounding regions accelerating toward them.\\nThese black holes are thought to have formed when massive stars reached the end of their cycle of evolution and collapsed under the influence of gravity.\\nIf a small volume of space contains enough mass, general relativity predicts that spacetime will become so highly curved that a black hole will form.\\ncosmic microwave background : The cosmic microwave background (CMB) radiation is electromagnetic radiation left over from when atoms first formed in the early universe, according to our standard model of cosmology.\\nPrior to that time, photons and the fundamental building blocks of matter formed a hot, dense soup, constantly interacting with one another.\\nAs the universe expanded and cooled, protons and neutrons formed atomic nuclei, which then combined with electrons to form neutral atoms.\\nAt this point, the photons effectively stopped interacting with them.\\nThese photons, which have stretched as the universe expanded, form the CMB.\\nFirst observed by Penzias and Wilson in 1965, the CMB remains the focus of increasingly precise observations intended to provide insight into the composition and evolution of the universe.\\nCoulomb\\'s Law : Coulomb\\'s Law states that the electric force between two charged particles is proportional to the product of the two charges divided by the square of the distance between the particles.\\nDoppler shift (Doppler effect) :\\nThe Doppler shift is a shift in the wavelength of light or sound that depends on the relative motion of the source and the observer.\\nA familiar example of a Doppler shift is the apparent change in pitch of an ambulance siren as it passes a stationary observer.\\nWhen the ambulance is moving toward the observer, the observer hears a higher pitch because the wavelength of the sound waves is shortened.\\nAs the ambulance moves away from the observer, the wavelength is lengthened and the observer hears a lower pitch.\\nLikewise, the wavelength of light emitted by an object moving toward an observer is shortened, and the observer will see a shift to blue.\\nIf the light-emitting object is moving away from the observer, the light will have a longer wavelength and the observer will see a shift to red.\\nBy observing this shift to red or blue, astronomers can determine the velocity of distant stars and galaxies relative to the Earth.\\nAtoms moving relative to a laser also experience a Doppler shift, which must be taken into account in atomic physics experiments that make use of laser cooling and trapping.\\nUnit 3: Gravity 42 www.learner.orgether: In the late nineteenth century, physicists were putting what they thought were the finishing touches on their theoretical description of electricity and magnetism.\\nIn the theory, electromagnetic waves traveled through a medium called \"luminiferous ether\" just as sound waves travel through the air, or the seismic waves that we experience as earthquakes travel through the Earth.\\nThe last remaining detail was to detect the ether and understand its properties.\\nIn 1887, Albert Michelson and Edward Morley performed an experiment, verified by many others, that demonstrated that light does not travel through ether.\\nThe lack of ether was one of many factors leading Einstein to develop special relativity.\\nevent horizon : A black hole\\'s event horizon is the point of no return for matter falling toward the black hole.\\nOnce matter enters the event horizon, it is gravitationally bound to the black hole and cannot escape.\\nHowever, an external observer will not see the matter enter the black hole.\\nInstead, the gravitational redshift due to the black hole\\'s strong gravitational field causes the object to appear to approach the horizon increasingly slowly without ever going beyond it.\\nWithin the event horizon, the black hole\\'s gravitational field warps spacetime so much that even light cannot escape.\\ngeneral relativity : General relativity is the theory Einstein developed to reconcile gravity with special relativity.\\nWhile special relativity accurately describes the laws of physics in inertial reference frames, it does not describe what happens in an accelerated reference frame or gravitational field.\\nSince acceleration and gravity are important parts of our physical world, Einstein recognized that special relativity was an incomplete description and spent the years between 1905 and 1915 developing general relativity.\\nIn general relativity, we inhabit a four-dimensional spacetime with a curvature determined by the distribution of matter and energy in space.\\nGeneral relativity makes unique, testable predictions that have been upheld by experimental measurements, including the precession of Mercury\\'s orbit, gravitational lensing, and gravitational time dilation.\\nOther predictions of general relativity, including gravitational waves, have not yet been verified.\\nWhile there is no direct experimental evidence that conflicts with general relativity, the accepted view is that general relativity is an approximation to a more fundamental theory of gravity that will unify it with the Standard Model.\\ngravitational time dilation, gravitational wave, precession, spacetime, special relativity, Standard Model.\\ngravitational lensing : Gravitational lensing occurs when light travels past a very massive object.\\nAccording to Einstein\\'s theory of general relativity, mass shapes spacetime and space is curved by massive objects.\\nLight traveling past a massive object follows a \"straight\" path in the curved space, and is deflected as if it had passed through a lens.\\nStrong gravitational lensing can cause stars to appear as rings as their light travels in a curved path past a massive object along the line of sight.\\nWe observe microlensing when an Unit 3: Gravity 43 www.learner.orgobject such as a MACHO moves between the Earth and a star.\\nThe gravitational lens associated with the MACHO focuses the star\\' light, so we observe the star grow brighter then dimmer as the MACHO moves across our line of sight to the star.\\ngravitational mass : The gravitational mass of a particle is the gravitational equivalent of electric charge: the physical property of an object that causes it to interact with other objects through the gravitational force.\\nAccording to the equivalence principle, gravitational mass is equivalent to inertial mass.\\nSee: equivalence principle, inertial mass.\\ngravitational time dilation : Clocks in a strong gravitational field run slower than clocks in a weaker gravitational field.\\nThis effect, predicted by Einstein\\'s theory of general relativity and confirmed by precision experiments both on Earth and in space, is called \"gravitational time dilation.\\n\" Hertz: Hertz (Hz) is a unit of frequency, defined as the number of complete cycles of a periodic signal that take place in one second.\\nFor example, the frequency of sound waves is usually reported in units of Hertz.\\nThe normal range of human hearing is roughly 20–20,000 Hz.\\nRadio waves have frequencies of thousands of Hz, and light waves in the visible part of the spectrum have frequencies of over 1014 Hz. inertial mass : Inertia is the measure of an object\\'s reluctance to accelerate under an applied force.\\nThe inertial mass of an object is the mass that appears in Newton\\'s second law: the acceleration of an object is equal to the applied force divided by its inertial mass.\\nThe more inertial mass an object has, the less it accelerates under a fixed applied force.\\nSee: equivalence principle, gravitational mass.\\nMOND: MOND, or Modified Newtonian Dynamics, is a theory that attempts to explain the evidence for dark matter as a modification to Newtonian gravity.\\nThere are many versions of the theory, all based on the premise that Newton\\'s laws are slightly different at very small accelerations.\\nA ball dropped above the surface of the Earth would not deviate noticeably from the path predicted by Newtonian physics, but the stars at the very edges of our galaxy would clearly demonstrate modified dynamics if MOND were correct.\\nNewton\\'s law of universal gravitation : Newton\\'s law of universal gravitation states that the gravitational force between two massive particles is proportional to the product of the two masses divided by the square of the distance between them.\\nThe law of universal gravitation is sometimes called the \"inverse square law.\"\\nSee: universal gravitational constant.\\npolarization : The polarization of a wave is the direction in which it is oscillating.\\nThe simplest type of polarization is linear, transverse polarization.\\nLinear means that the wave oscillation is confined Unit 3:\\nGravity 44 www.learner.orgalong a single axis, and transverse means that the wave is oscillating in a direction perpendicular to its direction of travel.\\nLaser light is most commonly a wave with linear, transverse polarization.\\nIf the laser beam travels along the x-axis, its electric field will oscillate either in the y-direction or in the z-direction.\\nGravitational waves also have transverse polarization, but have a more complicated oscillation pattern than laser light. precession : Precession is a systematic change in the orientation of a rotation axis.\\nFor example, the orbits of planets in our solar system precess.\\nEach planet follows an elliptical path around the Sun, with the Sun at one of the focal points of the ellipse.\\nThe long axis of the ellipse slowly rotates in the plane of the orbit with the Sun as a pivot point, so the planet never follows exactly the same path through space as it continues to orbit in its elliptical path.\\nThe precession measured in Mercury\\'s orbit was found to be different from the prediction of Newtonian gravity but matched the prediction of general relativity, providing some of the first concrete evidence that Einstein\\'s version of gravity is correct.\\nA pulsar is a spinning neutron star with a strong magnetic field that emits electromagnetic radiation along its magnetic axis.\\nBecause the star\\'s rotation axis is not aligned with its magnetic axis, we observe pulses of radiation as the star\\'s magnetic axis passes through our line of sight.\\nThe time between pulses ranges from a few milliseconds to a few seconds, and tends to slow down over time.\\nspacetime : In classical physics, space and time are considered separate things.\\nSpace is three- dimensional, and can be divided into a three-dimensional grid of cubes that describes the Euclidean geometry familiar from high-school math class.\\nTime is one-dimensional in classical physics.\\nEinstein\\'s theory of special relativity combines the three dimensions of space and one dimension of time into a four- dimensional grid called \"spacetime.\"\\nSpacetime may be flat, in which case Euclidean geometry describes the three space dimensions, or curved.\\nIn Einstein\\'s theory of general relativity, the distribution of matter and energy in the universe determines the curvature of spacetime.\\nspecial relativity : Einstein developed his theory of special relativity in 1905, 10 years before general relativity.\\nSpecial relativity is predicated on two postulates.\\nFirst, the speed of light is assumed to be constant in all inertial frames.\\nSecond, the laws of physics are assumed to be the same in all inertial frames.\\nAn inertial frame, in this context, is defined as a reference frame that is not accelerating or in a gravitational field.\\nStarting from these two postulates, Einstein derived a number of counterintuitive consequences that were later verified by experiment.\\nAmong them are time dilation (a moving clock will run slower than a stationary clock), length contraction (a moving ruler will be shorter than a stationary Unit 3: Gravity 45 www.learner.orgruler), the equivalence of mass and energy, and that nothing can move faster than the speed of light.\\nSee: general relativity, spacetime. standard model of cosmology :\\nOur best model for how the universe began and evolved into what we observe now is called the \"standard model of cosmology.\"\\nIt contends that the universe began in a Big Bang around 14 billion years ago, which was followed by a short period of exponential inflation.\\nAt the end of inflation, quarks, photons, and other fundamental particles formed a hot, dense soup that cooled as the universe continued to expand.\\nRoughly 390,000 years after the end of inflation, the first atoms formed and the cosmic microwave background photons decoupled.\\nOver the course of billions of years, the large structures and astronomical objects we observe throughout the cosmos formed as the universe continued to expand.\\nEventually the expansion rate of the universe started to increase under the influence of dark energy.\\ntorsion pendulum : A conventional pendulum is a mass suspended on a string that swings periodically.\\nA torsion pendulum is a mass suspended on a string (or torsion fiber) that rotates periodically.\\nWhen the mass of a torsion pendulum is rotated from its equilibrium position, the fiber resists the rotation and provides a restoring force that causes the mass to rotate back to its original equilibrium position.\\nWhen the mass reaches its equilibrium position, it is moving quickly and overshoots.\\nThe fiber\\'s restoring force, which is proportional to the rotation angle of the mass, eventually causes the mass to slow down and rotate back the other way.\\nBecause the restoring force of the torsion fiber is very small, a torsion pendulum can be used to measure extremely small forces affecting the test mass.\\nuniversal gravitational constant : The universal gravitational constant, denoted by G, is the proportionality constant in Newton\\'s law of universal gravitation.\\nThe currently accepted value for G is 6.67428±0.00067 x 10-11 N-m2/kg2. universality of free fall : The universality of free fall, sometimes abbreviated UFF, is the idea that all materials fall at the same rate in a uniform gravitational field.\\nThis is equivalent to stating that inertial and gravitational mass are the same.\\nSee: equivalence principle, gravitational mass, inertial mass.', -8.94944), ('A pulsar is a spinning neutron star with a strong magnetic field that emits electromagnetic radiation along its magnetic axis.', -11.065809), (\"The gravitational lens associated with the MACHO focuses the star' light, so we observe the star grow brighter then dimmer as the MACHO moves across our line of sight to the star.\", -11.095807), ('Unit 3: Gravity 6 www.learner.org Figure 3: Hubble Space Telescope image of a star-forming region in the Small Magellanic Cloud.', -11.172907), (\"Because the star's rotation axis is not aligned with its magnetic axis, we observe pulses of radiation as the star's magnetic axis passes through our line of sight.\", -11.225848), ('Scientists have made progress in recent years, but they still have much to learn.', -11.26859)]\n"
     ]
    }
   ],
   "source": [
    "results = agent.db.search_and_rank(\"Quiet Star Learning\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiet-STar learning refers to a specific type of machine learning technique known as Quantum Iterative Deepening (QID). This method involves computing and updating the amplitude distribution of quantum states in a quantum computer to represent the probability distribution of the desired solution. The \"Quiet\" aspect of Quiet-STar learning indicates the reduction of quantum error and decoherence during the computation process, and \"STar\" stands for \"Superposition Quantum Training at Rapid convergence.\" This technique leverages quantum computing principles to perform machine learning tasks with the potential for improved efficiency and computational power compared to classical machine learning algorithms. However, it should be noted that the practical implementation of Quiet-STar learning is still an area of active research and development within the field of quantum machine learning.\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(query=\"What is Quiet-STar learning?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
